[
    {
        "prefix": "import copy\nimport random\nimport matplotlib.pyplot as plt\n\nbest_fitness = 0\nbest_trajectory = []\nbest_number_of_steps = 0\nbest_number_of_treasures_found = 0\nbest_gameplan = [[0]*7]*7\n\ndef find_instructions(memory): #machine\n    instructions = []\n    index = 0\n    max_instructions = 0\n    # while len(instructions) < 16 and index < 64:\n    while index < 64 and max_instructions < 500:\n        max_instructions -=- 1\n        byte_value = memory[index]\n\n        byte_format = format(byte_value, '08b')\n        instruction_type = int(byte_format[:2], 2)  # first 2 numbers to get the instruction type\n        address = int(byte_format[2:], 2)  # get the last 6 numbers to get address refer\n\n        if instruction_type == 0:  # if 00, increment adress\n            memory[address] += 1\n            if memory[address] > 255:\n                memory[address] -= 255\n            index += 1\n        eli",
        "middle": "f instruction_type == 1",
        "suffix": ":  # if 01, decrement adress\n            memory[address] -= 1\n            if memory[address] < 0:\n                memory[address] += 255\n            index += 1\n        elif instruction_type == 2:  # if 10, jump on adress\n            index = byte_value\n        elif instruction_type == 3:  # if 11, append move\n            bin_number = bin(memory[index])  # convert number to binary\n            ones_count = bin_number.count('1')  # count how many ones are there\n\n            if 0 < ones_count < 3:\n                instructions.append(\"H\")\n            elif 2 < ones_count < 5:\n                instructions.append(\"D\")\n            elif 4 < ones_count < 7:\n                instructions.append(\"P\")\n            elif 6 < ones_count < 9:\n                instructions.append(\"L\")\n\n            index += 1\n            if index > 60:\n                index = random.randint(0,60)\n\n    return instructions\ndef generate_individual(first_population_fill):\n    memory = [0]*64 #64 memory cells with size of 1 byte\n    for m in range(first_population_fill):\n        memory[m] = random.randint(1,255) #fill with random values to get movement\n    return memory\n\ndef take_a_walk(trajectory):\n    gameplan = [\n        [\"o\", \"o\", \"o\", \"o\", \"o\", \"o\", \"o\"],\n        [\"o\", \"o\", \"o\", \"o\", \"P\", \"o\", \"o\"],\n        [\"o\", \"o\", \"P\", \"o\", \"o\", \"o\", \"o\"],\n        [\"o\", \"o\", \"o\", \"o\", \"o\", \"o\", \"P\"],\n        [\"o\", \"P\", \"o\", \"o\", \"o\", \"o\", \"o\"],\n        [\"o\", \"o\", \"o\", \"o\", \"P\", \"o\", \"o\"],\n        [\"o\", \"o\", \"o\", \"x\", \"o\", \"o\", \"o\"]\n    ]\n\n    steps_count = 0\n    treasues_found_count = 0\n    position_x = 6\n    pos_y = 3\n    for step in trajectory:\n        if treasues_found_count == 5:\n            return steps_count, treasues_found_count, gameplan\n\n        steps_count += 1\n        if step == \"H\":\n            position_x -= 1\n        elif step == \"D\":\n            position_x += 1\n        elif step == \"P\":\n            pos_y += 1\n        elif step == \"L\":\n            pos_y -= 1\n\n        #check if the move was succesfull and update the map + add +1 step and if found treasure, +1 treasure\n        if position_x < 0 or position_x > 6 or pos_y > 6 or pos_y < 0:  # fall out of map\n            # return steps_count, -3000, gameplan\n            return steps_count, -3, gameplan\n        else:  # did not fall\n            if gameplan[position_x][pos_y] == \"o\" or gameplan[position_x][pos_y] == \"x\":\n                gameplan[position_x][pos_y] = \"x\"\n            elif gameplan[position_x][pos_y] == \"P\":\n                treasues_found_count += 1\n                gameplan[position_x][pos_y] = \"x\"\n\n\n    return steps_count, treasues_found_count, gameplan\ndef fitness(indiv):\n    indiv_copy = indiv[:]\n    trajectory = find_instructions(indiv_copy)\n\n    global best_fitness\n    global best_trajectory\n    global best_number_of_steps\n    global best_number_of_treasures_found\n    global best_gameplan\n\n    steps_count, treasues_found_count, gameplan = take_a_walk(trajectory)\n\n    if treasues_found_count > 2:\n        fitness_score = treasues_found_count + (0.1000 - (steps_count / 500)) #swap so fewer steps has more points\n        if best_fitness < fitness_score:#replace the best individual\n            best_fitness = fitness_score\n            best_trajectory = trajectory[:]\n            best_number_of_steps = steps_count\n            best_number_of_treasures_found = treasues_found_count\n            best_gameplan = gameplan[:]\n        return fitness_score\n    elif treasues_found_count < 3:\n        fitness_score = treasues_found_count + (steps_count/1000) #swap so fewer steps has more points\n        if best_fitness < fitness_score: #replace the best individual\n            best_fitness = fitness_score\n            best_trajectory = trajectory[:]\n            best_number_of_steps = steps_count\n            best_number_of_treasures_found = treasues_found_count\n            best_gameplan = gameplan[:]\n        return fitness_score\n\ndef tournament_selection(ranked_population, tournament_size):\n\n    #select few random individuals for the turnament\n    tournament = random.sample(ranked_population, tournament_size)\n\n    #sort them by fitness\n    tournament.sort(key=lambda x: x[0], reverse=True)\n\n    #return the best individual from tournament\n    return tournament[0][1] #[0][1] because it is a tuple\ndef mutate(individual_parent):\n\n    mutation_type = random.choice([0, 1, 2])\n\n    if mutation_type == 0:\n        for i in range(2): #change random number in individual\n            random_index = random.randint(5, 60)\n            random_value = random.randint(1, 254)\n\n            individual_parent[random_index] = random_value\n    elif mutation_type == 1: #switch random bit\n        random_index = random.randint(0, 63)\n        bit_to_flip = random.randint(0, 7)  # Random bit to flip (0 to 7 for an 8-bit)\n\n        individual_parent[random_index] ^= (1 << bit_to_flip)  #use XOR to flip that bit\n    elif mutation_type == 2: # swap two random numbers\n        index1 = random.randint(0, 63)\n        index2 = random.randint(0, 63)\n\n        individual_parent[index1], individual_parent[index2] = individual_parent[index2], individual_parent[index1]\n\n    return individual_parent\n\n\ndef crossover(parent1, parent2):\n    crossover_point = random.randint(20, 40) #random number to split the genes of parents\n\n    #create two childrens witch oposite genes\n    child = parent1[:crossover_point] + parent2[crossover_point:]\n    child2 = parent2[:crossover_point] + parent1[crossover_point:]\n\n    return child, child2\n\ndef print_fitness_values_as_graph(fitness_values):\n    plt.figure(figsize=(10, 6))\n    plt.plot(fitness_values, color='blue', label='Fitness')\n    plt.xlabel('Generations', fontsize=14)\n    plt.ylabel('Fitness Value', fontsize=14)\n    plt.title('Fitness Progress Over Generations', fontsize=16)\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\n\n\n##### main()\n# genetic algrotithm variables setting\npopulation_size = 100\nelitism_count = 2\nmutation_rate = 5 #in %\nmax_no_update = 250 # maximum no improvement generations limit\n\ntournament_size = 3\nfirst_population_fill = 31\n\n# necessary variables\npopulation = []\nfitness_values_for_graph = []\n\n#fill first population\nfor i in range(population_size): #number of individuals in population\n    population.append(generate_individual(first_population_fill))\n\nprint(\"working on it...\")\n\nbreak_from_main = False\n# for i in range(500): #genetic algorithm (for test purposes)\nwhile not break_from_main:\n    ranked = []\n    best_fitness_for_generation = -100\n    for individual in population: #fitness(individual) to get the fitness score of the individual and also add individual to not lose it\n        individual_copy = individual.copy()\n        fitness_value = fitness(individual_copy)\n        ranked.append((fitness_value, individual))\n\n        if fitness_value > best_fitness_for_generation:\n            best_fitness_for_generation = fitness_value\n\n\n    fitness_values_for_graph.append(best_fitness_for_generation)\n\n    index = len(fitness_values_for_graph) - 1\n    exit_from_loop = 0\n\n    # check if to break from the loop or no\n    if len(fitness_values_for_graph) > max_no_update:\n        index = len(fitness_values_for_graph) - 1\n        exit_from_loop = 0\n\n        for check in range(max_no_update):\n            if fitness_values_for_graph[index] == fitness_values_for_graph[-1]:\n                exit_from_loop += 1\n            else:\n                exit_from_loop = 0  # Reset if any difference\n            index -= 1\n\n            if exit_from_loop == max_no_update:\n                print(f\"=no improvement in last {max_no_update} generations: exit from loop=\")\n                break_from_main = True\n                break\n\n    #elitism\n    ranked.sort(key=lambda x: x[0], reverse=True)\n    elites = [copy.deepcopy(ranked[r][1]) for r in range(elitism_count)] #carry over the best individuals to next generation\n\n    new_population = []\n    #tournament selection\n    # print_gameplan(gameplan)\n    while len(new_population) < (population_size - elitism_count):\n        #choose parents by turnament selection\n        ranked_copy = ranked[:]\n        parent1 = tournament_selection(ranked_copy, tournament_size)\n        parent2 = tournament_selection(ranked_copy, tournament_size)\n\n        parent1_copy, parent2_copy = parent1[:], parent2[:]\n        child, child2 = crossover(parent1_copy, parent2_copy)\n        child = mutate(child.copy())\n        child2 = mutate(child2.copy())\n\n        #mutate\n        if random.random() < (mutation_rate/100): # % chance of mutation\n            child = mutate(child.copy())\n        if random.random() < (mutation_rate/100): # % chance of mutation\n            child2 = mutate(child2.copy())\n\n        #add children to another generation\n        new_population.append(child)\n        new_population.append(child2)\n\n    #replace the population with the new one\n    population = elites.copy() + new_population.copy() #also ensure that we don't exceed population size\n\n\nprint(\"===========================\")\nprint(\"===========================\")\nprint(f\"The best program ended with fitness value of {best_fitness}\")\nprint(f\"It took {best_number_of_steps} steps\")\nprint(f\"And found {best_number_of_treasures_found} treasures\")\nprint(\"The map:\")\nfor row in best_gameplan:\n    print(row)\nprint(\"The moves:\")\nprint(best_trajectory)\n\nprint_fitness_values_as_graph(fitness_values_for_graph)\n# print(fitness_values_for_graph) # in case we want to use the data from graph such as for another script that combines multiple graphs (such as those in documentation)\n\n"
    },
    {
        "prefix": "import copy\nimport random\nimport matplotlib.pyplot as plt\n\nbest_fitness = 0\nbest_trajectory = []\nbest_number_of_steps = 0\nbest_number_of_treasures_found = 0\nbest_gameplan = [[0]*7]*7\n\ndef find_instructions(memory): #machine\n    instructions = []\n    index = 0\n    max_instructions = 0\n    # while len(instructions) < 16 and index < 64:\n    while index < 64 and max_instructions < 500:\n        max_instructions -=- 1\n        byte_value = memory[index]\n\n        byte_format = format(byte_value, '08b')\n        instruction_type = int(byte_format[:2], 2)  # first 2 numbers to get the instruction type\n        address = int(byte_format[2:], 2)  # get the last 6 numbers to get address refer\n\n        if instruction_type == 0:  # if 00, increment adress\n            memory[address] += 1\n            if memory[address] > 255:\n                memory[address] -= 255\n            index += 1\n        elif instruction_type == 1:  # if 01, decrement adress\n            memory[address] -= 1\n            if memory[address] < 0:\n                memory[address] += 255\n            index += 1\n        elif instruction_type == 2:  # if 10, jump on adress\n            index = byte_value\n        elif instruction_type == 3:  # if 11, append move\n            bin_number = bin(memory[index])  # convert number to binary\n            ones_count = bin_number.count('1')  # count how many ones are there\n\n            if 0 < ones_count < 3:\n                instructions.append(\"H\")\n            elif 2 < ones_count < 5:\n                instructions.append(\"D\")\n            elif 4 < ones_count < 7:\n                instructions.append(\"P\")\n            elif 6 < ones_count < 9:\n                instructions.append(\"L\")\n\n            index += 1\n            if index > 60:\n                index = random.randint(0,60)\n\n    return instructions\ndef generate_individual(first_population_fill):\n    memory = [0]*64 #64 memory cells with size of 1 byte\n    for m in range(first_population_fill):\n        memory[m] = random.randint(1,255) #fill with random values to get movement\n    return memory\n\ndef take_a_walk(trajectory):\n    gameplan = [\n        [\"o\", \"o\", \"o\", \"o\", \"o\", \"o\", \"o\"],\n        [\"o\", \"o\", \"o\", \"o\", \"P\", \"o\", \"o\"],\n        [\"o\", \"o\", \"P\", \"o\", \"o\", \"o\", \"o\"],\n        [\"o\", \"o\", \"o\", \"o\", \"o\", \"o\", \"P\"],\n        [\"o\", \"P\", \"o\", \"o\", \"o\", \"o\", \"o\"],\n        [\"o\", \"o\", \"o\", \"o\", \"P\", \"o\", \"o\"],\n        [\"o\", \"o\", \"o\", \"x\", \"o\", \"o\", \"o\"]\n    ]\n\n    steps_count = 0\n    treasues_found_count = 0\n    position_x = 6\n    pos_y = 3\n    for step in trajectory:\n        if treasues_found_count == 5:\n            return steps_count, treasues_found_count, gameplan\n\n        steps_count += 1\n        if step == \"H\":\n            position_x -= 1\n        elif step == \"D\":\n            position_x += 1\n        elif step == \"P\":\n            pos_y += 1\n        elif step == \"L\":\n            pos_y -= 1\n\n        #check if the move was succesfull and update the map + add +1 step and if found treasure, +1 treasure\n        if position_x < 0 or position_x > 6 or pos_y > 6 or pos_y < 0:  # fall out of map\n            # return steps_count, -3000, gameplan\n            return steps_count, -3, gameplan\n        else:  # did not fall\n            if gameplan[position_x][pos_y] == \"o\" or gameplan[position_x][pos_y] == \"x\":\n                gameplan[position_x][pos_y] = \"x\"\n            elif gameplan[position_x][pos_y] == \"P\":\n                treasues_found_count += 1\n                gameplan[position_x][pos_y] = \"x\"\n\n\n    return steps_count, treasues_found_count, gameplan\ndef fitness(indiv):\n    indiv_copy = indiv[:]\n    trajectory = find_instructions(indiv_copy)\n\n    global best_fitness\n    global best_trajectory\n    global best_number_of_steps\n    global best_number_of_treasures_found\n    global best_gameplan\n\n    steps_count, treasues_found_count, gameplan = take_a_walk(trajectory)\n\n    if treasues_found_count > 2:\n        fitness_score = treasues_found_count + (0.1000 - (steps_count / 500)) #swap so fewer steps has more points\n        if best_fitness < fitness_score:#replace the best individual\n            best_fitness = fitness_score\n            best_trajectory = trajectory[:]\n            best_number_of_steps = steps_count\n            best_number_of_treasures_found = treasues_found_count\n            best_gameplan = gameplan[:]\n        return fitness_score\n    elif treasues_found_count < 3:\n        fitness_score = treasues_found_count + (steps_count/1000) #swap so fewer steps has more points\n        if best_fitness < fitness_score: #replace the best individual\n            best_fitness = fitness_score\n            best_trajectory = trajectory[:]\n            best_number_of_steps = steps_count\n            best_number_of_treasures_found = treasues_found_count\n            best_gameplan = gameplan[:]\n        return fitness_score\n\ndef tournament_selection(ranked_population, tournament_size):\n\n    #select few random individuals for the turnament\n    tournament = random.sample(ranked_population, tournament_size)\n\n    #sort them by fitness\n    tournament.sort(key=lambda x: x[0], reverse=True)\n\n    #return the best individual from tournament\n    return tournament[0][1] #[0][1] because it is a tuple\ndef mutate(individual_parent):\n\n    mutation_type = random.choice([0, 1, 2])\n\n    if mutation_type == 0:\n        for i in range(2): #change random number in individual\n            random_index = random.randint(5, 60)\n            random_value = random.randint(1, 254)\n\n            individual_parent[random_index] = random_value\n    elif mutation_type == 1: #switch random bit\n        random_index = random.randint(0, 63)\n        bit_to_flip = random.randint(0, 7)  # Random bit to flip (0 to 7 for an 8-bit)\n\n        individual_parent[random_index] ^= (1 << bit_to_flip)  #use XOR to flip that bit\n    elif mutation_type == 2: # swap two random numbers\n        index1 = random.randint(0, 63)\n        index2 = random.randint(0, 63)\n\n        individual_parent[index1], individual_parent[index2] = individual_parent[index2], individual_parent[index1]\n\n    return individual_parent\n\n\ndef crossover(parent1, parent2):\n    crossover_point = random.randint(20, 40) #random number to split the genes of parents\n\n    #create two childrens witch oposite genes\n    child = parent1[:crossover_point] + parent2[crossover_point:]\n    child2 = parent2[:crossover_point] + parent1[crossover_point:]\n\n    return child, child2\n\ndef print_fitness_values_as_graph(fitness_values):\n    plt.figure(figsize=(10, 6))\n    plt.plot(fitness_values, color='blue', label='Fitness')\n    plt.xlabel('Generations', fontsize=14)\n    plt.ylabel('Fitness Value', fontsize=14)\n    plt.title('Fitness Progress Over Generations', fontsize=16)\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\n\n\n##### main()\n# genetic algrotithm variables setting\npopulation_size = 100\nelitism_count = 2\nmutation_rate = 5 #in %\nmax_no_update = 250 # maximum no improvement generations limit\n\ntournament_size = 3\nfirst_population_fill = 31\n\n# necessary variables\npopulation = []\nfitness_values_for_graph = []\n\n#fill first population\nfor i in range(population_size): #number of individuals in population\n    population.append(generate_individual(first_population_fill))\n\nprint(\"working on it...\")",
        "middle": "\n\nbreak_from_mai",
        "suffix": "n = False\n# for i in range(500): #genetic algorithm (for test purposes)\nwhile not break_from_main:\n    ranked = []\n    best_fitness_for_generation = -100\n    for individual in population: #fitness(individual) to get the fitness score of the individual and also add individual to not lose it\n        individual_copy = individual.copy()\n        fitness_value = fitness(individual_copy)\n        ranked.append((fitness_value, individual))\n\n        if fitness_value > best_fitness_for_generation:\n            best_fitness_for_generation = fitness_value\n\n\n    fitness_values_for_graph.append(best_fitness_for_generation)\n\n    index = len(fitness_values_for_graph) - 1\n    exit_from_loop = 0\n\n    # check if to break from the loop or no\n    if len(fitness_values_for_graph) > max_no_update:\n        index = len(fitness_values_for_graph) - 1\n        exit_from_loop = 0\n\n        for check in range(max_no_update):\n            if fitness_values_for_graph[index] == fitness_values_for_graph[-1]:\n                exit_from_loop += 1\n            else:\n                exit_from_loop = 0  # Reset if any difference\n            index -= 1\n\n            if exit_from_loop == max_no_update:\n                print(f\"=no improvement in last {max_no_update} generations: exit from loop=\")\n                break_from_main = True\n                break\n\n    #elitism\n    ranked.sort(key=lambda x: x[0], reverse=True)\n    elites = [copy.deepcopy(ranked[r][1]) for r in range(elitism_count)] #carry over the best individuals to next generation\n\n    new_population = []\n    #tournament selection\n    # print_gameplan(gameplan)\n    while len(new_population) < (population_size - elitism_count):\n        #choose parents by turnament selection\n        ranked_copy = ranked[:]\n        parent1 = tournament_selection(ranked_copy, tournament_size)\n        parent2 = tournament_selection(ranked_copy, tournament_size)\n\n        parent1_copy, parent2_copy = parent1[:], parent2[:]\n        child, child2 = crossover(parent1_copy, parent2_copy)\n        child = mutate(child.copy())\n        child2 = mutate(child2.copy())\n\n        #mutate\n        if random.random() < (mutation_rate/100): # % chance of mutation\n            child = mutate(child.copy())\n        if random.random() < (mutation_rate/100): # % chance of mutation\n            child2 = mutate(child2.copy())\n\n        #add children to another generation\n        new_population.append(child)\n        new_population.append(child2)\n\n    #replace the population with the new one\n    population = elites.copy() + new_population.copy() #also ensure that we don't exceed population size\n\n\nprint(\"===========================\")\nprint(\"===========================\")\nprint(f\"The best program ended with fitness value of {best_fitness}\")\nprint(f\"It took {best_number_of_steps} steps\")\nprint(f\"And found {best_number_of_treasures_found} treasures\")\nprint(\"The map:\")\nfor row in best_gameplan:\n    print(row)\nprint(\"The moves:\")\nprint(best_trajectory)\n\nprint_fitness_values_as_graph(fitness_values_for_graph)\n# print(fitness_values_for_graph) # in case we want to use the data from graph such as for another script that combines multiple graphs (such as those in documentation)\n\n"
    },
    {
        "prefix": "import copy\nimport random\nimport matplotlib.pyplot as plt\n\nbest_fitness = 0\nbest_trajectory = []\nbest_number_of_steps = 0\nbest_number_of_treasures_found = 0\nbest_gameplan = [[0]*7]*7\n\ndef find_instructions(memory): #machine\n    instructions = []\n    index = 0\n    max_instructions = 0\n    # while len(instructions) < 16 and index < 64:\n    while index < 64 and max_instructions < 500:\n        max_instructions -=- 1\n        byte_value = memory[index]\n\n        byte_format = format(byte_value, '08b')\n        instruction_type = int(byte_format[:2], 2)  # first 2 numbers to get the instruction type\n        address = int(byte_format[2:], 2)  # get the last 6 numbers to get address refer\n\n        if instruction_type == 0:  # if 00, increment adress\n            memory[address] += 1\n            if memory[address] > 255:\n                memory[address] -= 255\n            index += 1\n        elif instruction_type == 1:  # if 01, decrement adress\n            memory[address] -= 1\n            if memory[address] < 0:\n                memory[address] += 255\n            index += 1\n        elif instruction_type == 2:  # if 10, jump on adress\n            index = byte_value\n        elif instruction_type == 3:  # if 11, append move\n            bin_number = bin(memory[index])  # convert number to binary\n            ones_count = bin_number.count('1')  # count how many ones are there\n\n            if 0 < ones_count < 3:\n                instructions.append(\"H\")\n            elif 2 < ones_count < 5:\n                instructions.append(\"D\")\n            elif 4 < ones_count < 7:\n                instructions.append(\"P\")\n            elif 6 < ones_count < 9:\n                instructions.append(\"L\")\n\n            index += 1\n            if index > 60:\n                index = random.randint(0,60)\n\n    return instructions\ndef generate_individual(first_population_fill):\n    memory = [0]*64 #64 memory cells with size of 1 byte\n    for m in range(first_population_fill):\n        memory[m] = random.randint(1,255) #fill with random values to get movement\n    return memory\n\ndef take_a_walk(trajectory):\n    gameplan = [\n        [\"o\", \"o\", \"o\", \"o\", \"o\", \"o\", \"o\"],\n        [\"o\", \"o\", \"o\", \"o\", \"P\", \"o\", \"o\"],\n        [\"o\", \"o\", \"P\", \"o\", \"o\", \"o\", \"o\"],\n        [\"o\", \"o\", \"o\", \"o\", \"o\", \"o\", \"P\"],\n        [\"o\", \"P\", \"o\", \"o\", \"o\", \"o\", \"o\"],\n        [\"o\", \"o\", \"o\", \"o\", \"P\", \"o\", \"o\"],\n        [\"o\", \"o\", \"o\", \"x\", \"o\", \"o\", \"o\"]\n    ]\n\n    steps_count = 0\n    treasues_found_count = 0\n    position_x = 6\n    pos_y = 3\n    for step in trajectory:\n        if treasues_found_count == 5:\n            return steps_count, treasues_found_count, gameplan\n\n        steps_count += 1\n        if step == \"H\":\n            position_x -= 1\n        elif step == \"D\":\n            position_x += 1\n        elif step == \"P\":\n            pos_y += 1\n        elif step == \"L\":\n            pos_y -= 1\n\n        #check if the move was succesfull and update the map + add +1 step and if found treasure, +1 treasure\n        if position_x < 0 or position_x > 6 or pos_y > 6 or pos_y < 0:  # fall out of map\n            # return steps_count, -3000, gameplan\n            return steps_count, -3, gameplan\n        else:  # did not fall\n            if gameplan[position_x][pos_y] == \"o\" or gameplan[position_x][pos_y] == \"x\":\n                gameplan[position_x][pos_y] = \"x\"\n            elif gameplan[position_x][pos_y] == \"P\":\n                treasues_found_count += 1\n                gameplan[position_x][pos_y] = \"x\"\n\n\n    return steps_count, treasues_found_count, gameplan\ndef fitness(indiv):\n    indiv_copy = indiv[:]\n    trajectory = find_instructions(indiv_copy)\n\n    global best_fitness\n    global best_trajectory\n    global best_number_of_steps\n    global best_number_of_treasures_found\n    global best_gameplan\n\n    steps_count, treasues_found_count, gameplan = take_a_walk(trajectory)\n\n    if treasues_found_count > 2:\n        fitness_score = treasues_found_count + (0.1000 - (steps_count / 500)) #swap so fewer steps has more points\n        if best_fitness < fitness_score:#replace the best individual\n            best_fitness = fitness_score\n            best_trajectory = trajectory[:]\n            best_number_of_steps = steps_count\n            best_number_of_treasures_found = treasues_found_count\n            best_gameplan = gameplan[:]\n        return fitness_score\n    elif treasues_found_count < 3:\n        fitness_score = treasues_found_count + (steps_count/1000) #swap so fewer steps has more points\n        if best_fitness < fitness_score: #replace the best individual\n            best_fitness = fitness_score\n            best_trajectory = trajectory[:]\n            best_number_of_steps = steps_count\n            best_number_of_treasures_found = treasues_found_count\n            best_gameplan = gameplan[:]\n        return fitness_score\n\ndef tournament_selection(ranked_population, tournament_size):\n\n    #select few random individuals for the turnament\n    tournament = random.sample(ranked_population, tournament_size)\n\n    #sort them by fitness\n    tournament.sort(key=lambda x: x[0], reverse=True)\n\n    #return the best individual from tournament\n    return tournament[0][1] #[0][1] because it is a tuple\ndef mutate(individual_parent):\n\n    mutation_type = random.choice([0, 1, 2])\n\n    if mutation_type == 0:\n        for i in range(2): #change random number in individual\n            random_index = random.randint(5, 60)\n            random_value = random.randint(1, 254)\n\n            individual_parent[random_index] = random_value\n    elif mutation_type == 1: #switch random bit\n        random_index = random.randint(0, 63)\n        bit_to_flip = random.randint(0, 7)  # Random bit to flip (0 to 7 for an 8-bit)\n\n        individual_parent[random_index] ^= (1 << bit_to_flip)  #use XOR to flip that bit\n    elif mutation_type == 2: # swap two random numbers\n        index1 = random.randint(0, 63)\n        index2 = random.randint(0, 63)\n\n        individual_parent[index1], individual_parent[index2] = individual_parent[index2], individual_parent[index1]\n\n    return individual_parent\n\n\ndef crossover(parent1, parent2):\n    crossover_point = random.randint(20, 40) #random number to split the genes of parents\n\n    #create two childrens witch oposite genes\n    child = parent1[:crossover_point] + parent2[crossover_point:]\n    child2 = parent2[:crossover_point] + parent1[crossover_point:]\n\n    return child, child2\n\ndef print_fitness_values_as_graph(fitness_values):\n    plt.figure(figsize=(10, 6))\n    plt.plot(fitness_values, color='blue', label='Fitness')\n    plt.xlabel('Generations', fontsize=14)\n    plt.ylabel('Fitness Value', fontsize=14)\n    plt.title('Fitness Progress Over Generations', fontsize=16)\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\n\n\n##### main()\n# genetic algrotithm variables setting\npopulation_size = 100\nelitism_count = 2\nmutation_rate = 5 #in %\nmax_no_update = 250 # maximum no improvement generations limit\n\ntournament_size = 3\nfirst_population_fill = 31\n\n# necessary variables\npopulation = []\nfitness_values_for_graph = []\n\n#fill first population\nfor i in range(population_size): #number of individuals in population\n    population.append(generate_individual(first_population_fill))\n\nprint(\"working on it...\")\n\nbreak_from_main = False\n# for i in range(500): #genetic algorithm (for test purposes)\nwhile not break_from_main:\n    ranked = []\n    best_fitness_for_generation = -100\n    for individual in population: #fitness(individual) to get the fitness score of the individual and also add individual to not lose it\n        individual_copy = individual.copy()\n        fitness_value = fitness(individual_copy)\n        ranked.append((fitness_value, individual))\n\n        if fitness_value > best_fitness_for_generation:\n            best_fitness_for_generation = fitness_value\n\n\n    fitness_values_for_graph.append(best_fitness_for_generation)\n\n    index = len(fitness_values_for_graph) - 1\n    exit_from_loop = 0\n\n    # check if to break from the loop or no\n    if len(fitness_values_for_graph) > max_no_update:\n        index = len(fitness_values_for_graph) - 1\n        exit_from_loop = 0\n\n        for check in range(max_no_update):\n            if fitness_values_for_graph[index] == fitness_values_for_graph[-1]:\n                exit_from_loop += 1\n            else:\n                exit_from_loop = 0  # Reset if any difference\n            index -= 1\n\n            if exit_from_loop == max_no_update:\n                print(f\"=no improvement in last {max_no_update} generations: exit from loop=\")\n                break_from_main = True\n                break\n\n    #elitism\n    ranked.sort(key=lambda x: x[0], reverse=True)\n    elites = [copy.deepcopy(ranked[r][1]) for r in range(elitism_count)] #carry",
        "middle": " over the best individ",
        "suffix": "uals to next generation\n\n    new_population = []\n    #tournament selection\n    # print_gameplan(gameplan)\n    while len(new_population) < (population_size - elitism_count):\n        #choose parents by turnament selection\n        ranked_copy = ranked[:]\n        parent1 = tournament_selection(ranked_copy, tournament_size)\n        parent2 = tournament_selection(ranked_copy, tournament_size)\n\n        parent1_copy, parent2_copy = parent1[:], parent2[:]\n        child, child2 = crossover(parent1_copy, parent2_copy)\n        child = mutate(child.copy())\n        child2 = mutate(child2.copy())\n\n        #mutate\n        if random.random() < (mutation_rate/100): # % chance of mutation\n            child = mutate(child.copy())\n        if random.random() < (mutation_rate/100): # % chance of mutation\n            child2 = mutate(child2.copy())\n\n        #add children to another generation\n        new_population.append(child)\n        new_population.append(child2)\n\n    #replace the population with the new one\n    population = elites.copy() + new_population.copy() #also ensure that we don't exceed population size\n\n\nprint(\"===========================\")\nprint(\"===========================\")\nprint(f\"The best program ended with fitness value of {best_fitness}\")\nprint(f\"It took {best_number_of_steps} steps\")\nprint(f\"And found {best_number_of_treasures_found} treasures\")\nprint(\"The map:\")\nfor row in best_gameplan:\n    print(row)\nprint(\"The moves:\")\nprint(best_trajectory)\n\nprint_fitness_values_as_graph(fitness_values_for_graph)\n# print(fitness_values_for_graph) # in case we want to use the data from graph such as for another script that combines multiple graphs (such as those in documentation)\n\n"
    },
    {
        "prefix": "from matplotlib import pyplot as plt\nimport numpy\n\n# !FUNCTIONS!\n\ndef generate_start_points(num_points, lower_interval_bound, upper_interval_bound):\n    points_set = set() # use set so we have unique points\n\n    while len(points_set) < num_points: #untill points_set is not full - based on num_points\n        #generate random point (x,y)\n        x = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1) \n        y = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n\n        points_set.add((x, y)) #add x and y as a tuple into the set\n\n    return numpy.array(list(points_set))  # convert set to numpy array and return\ndef generate_additional_points(default_points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty):\n    points = default_points.copy() #copy numpy array and work with its copy\n\n    for _ in range(num_additional_points):\n        # choose random point from existing points\n        random_index = numpy.random.randint(len(points))\n        random_point = points[random_index]\n        x, y = random_point\n\n        # edit offset interval if too close to edge\n        offset_interval = 100\n        if  x < lower_interval_bound + 100 or x > upper_interval_bound-100 or y < lower_interval_bound + 100 or y > upper_interval_bound-100:\n            offset_interval -= close_to_edge_penalty\n\n        # generate offset\n        X_offset = numpy.random.randint(-offset_interval, offset_interval)\n        Y_offset = numpy.random.randint(-offset_interval, offset_interval)\n\n        # set new point\n        new_x = x+X_offset\n        new_y = y+Y_offset\n\n        # append new point to the points\n        points = numpy.append(points, numpy.array([[new_x, new_y]]), axis = 0)\n\n    return points\ndef visualise_clusters(clusters, medoids):\n    # colors to choose from\n    colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\", \"pink\", \"brown\", \"cyan\", \"magenta\", \"lime\", \"indigo\", \"violet\", \"gold\", \"silver\", \"turquoise\", \"maroon\", \"navy\", \"coral\", \"teal\"]\n    plt.figure(figsize=(10, 10))\n\n    # for each cluster, show one point on matplotlib\n    for i in range(len(clusters)):\n        cluster = clusters[i]\n        if len(cluster) > 0:\n            cluster_points = numpy.array(cluster)\n            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i % len(colors)], s=10, label='Cluster {}'.format(i+1))\n\n    # Plot centroids with larger size and different color\n    medoids = numpy.array(medoids)\n    plt.scatter(medoids[:, 0], medoids[:, 1], c='black', s=100, marker='x', label='Centroids', alpha=0.7)\n\n    # aditional matplotlib settings\n    plt.xlim(-5000, 5000)\n    plt.ylim(-5000, 5000)\n    plt.title(\"K-Means Clustering (medoid)\")\n    plt.xlabel(\"X Coordinate\")\n    plt.ylabel(\"Y Coordinate\")\n  ",
        "middle": "  plt.legend()\n    plt.grid(Fa",
        "suffix": "lse)\n    plt.show()\n\ndef euclidean_distance(point, other_point):\n    # calculate euclidian distance based on formula \n    return numpy.sqrt(numpy.sum(((other_point - point) ** 2)))\n\ndef k_means(points, k, lower_interval_bound, upper_interval_bound):\n    # choose k random medoids \n    medoids = []\n    for _ in range(k):\n        medoids.append([numpy.random.randint(lower_interval_bound, upper_interval_bound), numpy.random.randint(lower_interval_bound, upper_interval_bound)])\n\n    #asign points to the nearest centroid\n    i = 1\n    \n    current_avg = 0\n    previous_avg = 0\n\n    no_update_count = 0\n    average_distances = 0\n    average_distances_count = 0\n    while True:\n        print(f\"Iteration no.: {i}\")\n        i -=- 1\n\n        clusters = [[] for _ in range(k)] #define k clusters\n        distances_to_medoids = []\n\n        # put each point to cluster\n        for point in points:\n            all_distances = [euclidean_distance(point, medoid) for medoid in medoids]\n\n            average_distances += sum(all_distances)\n            average_distances_count += len(all_distances)\n\n            closest_medoid = numpy.argmin(all_distances)\n            distances_to_medoids.append(all_distances[closest_medoid])\n            clusters[closest_medoid].append(point)\n\n        # Calculate the average distance for this iteration\n        if distances_to_medoids:\n            average_distances = sum(distances_to_medoids)\n            average_distances_count = len(distances_to_medoids)\n            current_avg = average_distances / average_distances_count\n            print(f\"Average distance to closest medoid: {current_avg:.2f}\")\n\n        #recalculate medoids\n        new_medoids = []\n        for cluster in clusters:\n            if len(cluster) > 0:\n                cluster_mean = numpy.mean(cluster, axis=0)\n                medoid = min(cluster, key=lambda point: euclidean_distance(point, cluster_mean))\n                new_medoids.append(medoid)\n            else:\n                new_medoids.append(points[numpy.random.randint(len(points))])\n\n        if no_update_count > 0:\n            print(f\"No update for {no_update_count}/3\")\n\n        if abs(previous_avg - current_avg) < 1:\n            no_update_count += 1\n        else:\n            no_update_count = 0  # Reset if assignments have changed\n\n\n        # Stop if there's no update for 3 iterations\n        if no_update_count >= 3:\n            print(f\"    very small update for {no_update_count} iterations, not worth to continue\")\n            break\n\n        # Update centroids for the next iteration\n        medoids = new_medoids\n        previous_avg = current_avg\n\n    return clusters, medoids\n\ndef evaluate_cluster_average_mean(clusters, centroids, k):\n\n    succesful_clusters = 0\n\n    # calculate the sum of distances between each point in the cluster and the centroid of that cluster\n    for i, cluster in enumerate(clusters):\n        if len(cluster) > 0:\n            # Sum up the distances from each point in the cluster to its centroid\n            total_distance = sum(euclidean_distance(point, centroids[i]) for point in cluster)\n            avg_distance = total_distance / len(cluster)\n            if avg_distance <= 500:\n                succesful_clusters += 1\n            print(f\"Average distance for cluster {i + 1}: {avg_distance:.2f}\")\n        else: \n            print(f\"Cluster {i+1} was left empty\")\n\n    print(f\"Number of cluster with average distance from centroid smaller than 500 is {succesful_clusters} from {k} possible\")\n\n    return\n####\n\n \n# !START HERE!\n\n#important variables\nnum_points = 20\nlower_interval_bound = -5000\nupper_interval_bound = 5000\n\nnum_additional_points = 40000\nclose_to_edge_penalty = 10\n\nk = 15 #number of clusters\n\n#inicialize first points\npoints = generate_start_points(num_points, lower_interval_bound, upper_interval_bound) \n# find additional points\nadditional_points = generate_additional_points(points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty) \n\nclusters, medoids = k_means(additional_points, k, lower_interval_bound, upper_interval_bound)\nevaluate_cluster_average_mean(clusters, medoids, k)\nprint(\"buliding visualisation...\")\nvisualise_clusters(clusters, medoids)\n"
    },
    {
        "prefix": "from matplotlib import pyplot as plt\nimport numpy\n\n# !FUNCTIONS!\n\ndef generate_start_points(num_points, lower_interval_bound, upper_interval_bound):\n    points_set = set() # use set so we have unique points\n\n    while len(points_set) < num_points: #untill points_set is not full - based on num_points\n        #generate random point (x,y)\n        x = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1) \n        y = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n\n        points_set.add((x, y)) #add x and y as a tuple into the set\n\n    return numpy.array(list(points_set))  # convert set to numpy array and return\ndef generate_additional_points(default_points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty):\n    points = default_points.copy() #copy numpy array and work with its copy\n\n    for _ in range(num_additional_points):\n        # choose random point from existing points\n        random_index = numpy.random.randint(len(points))\n        random_point = points[random_index]\n        x, y = random_point\n\n        # edit offset interval if too close to edge\n        offset_interval = 100\n        if  x < lower_interval_bound + 100 or x > upper_interval_bound-100 or y < lower_interval_bound + 100 or y > upper_interval_bound-100:\n            offset_interval -= close_to_edge_penalty\n\n        # generate offset\n        X_offset = numpy.random.randint(-offset_interval, offset_interval)\n        Y_offset = numpy.random.randint(-offset_interval, offset_interval)\n\n        # set new point\n    ",
        "middle": "    new_x = x+X_offset\n      ",
        "suffix": "  new_y = y+Y_offset\n\n        # append new point to the points\n        points = numpy.append(points, numpy.array([[new_x, new_y]]), axis = 0)\n\n    return points\ndef visualise_clusters(clusters, medoids):\n    # colors to choose from\n    colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\", \"pink\", \"brown\", \"cyan\", \"magenta\", \"lime\", \"indigo\", \"violet\", \"gold\", \"silver\", \"turquoise\", \"maroon\", \"navy\", \"coral\", \"teal\"]\n    plt.figure(figsize=(10, 10))\n\n    # for each cluster, show one point on matplotlib\n    for i in range(len(clusters)):\n        cluster = clusters[i]\n        if len(cluster) > 0:\n            cluster_points = numpy.array(cluster)\n            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i % len(colors)], s=10, label='Cluster {}'.format(i+1))\n\n    # Plot centroids with larger size and different color\n    medoids = numpy.array(medoids)\n    plt.scatter(medoids[:, 0], medoids[:, 1], c='black', s=100, marker='x', label='Centroids', alpha=0.7)\n\n    # aditional matplotlib settings\n    plt.xlim(-5000, 5000)\n    plt.ylim(-5000, 5000)\n    plt.title(\"K-Means Clustering (medoid)\")\n    plt.xlabel(\"X Coordinate\")\n    plt.ylabel(\"Y Coordinate\")\n    plt.legend()\n    plt.grid(False)\n    plt.show()\n\ndef euclidean_distance(point, other_point):\n    # calculate euclidian distance based on formula \n    return numpy.sqrt(numpy.sum(((other_point - point) ** 2)))\n\ndef k_means(points, k, lower_interval_bound, upper_interval_bound):\n    # choose k random medoids \n    medoids = []\n    for _ in range(k):\n        medoids.append([numpy.random.randint(lower_interval_bound, upper_interval_bound), numpy.random.randint(lower_interval_bound, upper_interval_bound)])\n\n    #asign points to the nearest centroid\n    i = 1\n    \n    current_avg = 0\n    previous_avg = 0\n\n    no_update_count = 0\n    average_distances = 0\n    average_distances_count = 0\n    while True:\n        print(f\"Iteration no.: {i}\")\n        i -=- 1\n\n        clusters = [[] for _ in range(k)] #define k clusters\n        distances_to_medoids = []\n\n        # put each point to cluster\n        for point in points:\n            all_distances = [euclidean_distance(point, medoid) for medoid in medoids]\n\n            average_distances += sum(all_distances)\n            average_distances_count += len(all_distances)\n\n            closest_medoid = numpy.argmin(all_distances)\n            distances_to_medoids.append(all_distances[closest_medoid])\n            clusters[closest_medoid].append(point)\n\n        # Calculate the average distance for this iteration\n        if distances_to_medoids:\n            average_distances = sum(distances_to_medoids)\n            average_distances_count = len(distances_to_medoids)\n            current_avg = average_distances / average_distances_count\n            print(f\"Average distance to closest medoid: {current_avg:.2f}\")\n\n        #recalculate medoids\n        new_medoids = []\n        for cluster in clusters:\n            if len(cluster) > 0:\n                cluster_mean = numpy.mean(cluster, axis=0)\n                medoid = min(cluster, key=lambda point: euclidean_distance(point, cluster_mean))\n                new_medoids.append(medoid)\n            else:\n                new_medoids.append(points[numpy.random.randint(len(points))])\n\n        if no_update_count > 0:\n            print(f\"No update for {no_update_count}/3\")\n\n        if abs(previous_avg - current_avg) < 1:\n            no_update_count += 1\n        else:\n            no_update_count = 0  # Reset if assignments have changed\n\n\n        # Stop if there's no update for 3 iterations\n        if no_update_count >= 3:\n            print(f\"    very small update for {no_update_count} iterations, not worth to continue\")\n            break\n\n        # Update centroids for the next iteration\n        medoids = new_medoids\n        previous_avg = current_avg\n\n    return clusters, medoids\n\ndef evaluate_cluster_average_mean(clusters, centroids, k):\n\n    succesful_clusters = 0\n\n    # calculate the sum of distances between each point in the cluster and the centroid of that cluster\n    for i, cluster in enumerate(clusters):\n        if len(cluster) > 0:\n            # Sum up the distances from each point in the cluster to its centroid\n            total_distance = sum(euclidean_distance(point, centroids[i]) for point in cluster)\n            avg_distance = total_distance / len(cluster)\n            if avg_distance <= 500:\n                succesful_clusters += 1\n            print(f\"Average distance for cluster {i + 1}: {avg_distance:.2f}\")\n        else: \n            print(f\"Cluster {i+1} was left empty\")\n\n    print(f\"Number of cluster with average distance from centroid smaller than 500 is {succesful_clusters} from {k} possible\")\n\n    return\n####\n\n \n# !START HERE!\n\n#important variables\nnum_points = 20\nlower_interval_bound = -5000\nupper_interval_bound = 5000\n\nnum_additional_points = 40000\nclose_to_edge_penalty = 10\n\nk = 15 #number of clusters\n\n#inicialize first points\npoints = generate_start_points(num_points, lower_interval_bound, upper_interval_bound) \n# find additional points\nadditional_points = generate_additional_points(points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty) \n\nclusters, medoids = k_means(additional_points, k, lower_interval_bound, upper_interval_bound)\nevaluate_cluster_average_mean(clusters, medoids, k)\nprint(\"buliding visualisation...\")\nvisualise_clusters(clusters, medoids)\n"
    },
    {
        "prefix": "from matplotlib import pyplot as plt\nimport numpy\n\n# !FUNCTIONS!\n\ndef generate_start_points(num_points, lower_interval_bound, upper_interval_bound):\n    points_set = set() # use set so we have unique points\n\n    while len(points_set) < num_points: #untill points_set is not full - based on num_points\n        #generate random point (x,y)\n        x = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1) \n        y = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n\n        points_set.add((x, y)) #add x and y as a tuple into the set\n\n    return numpy.array(list(points_set))  # convert set to numpy array and return\ndef generate_additional_points(default_points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty):\n    points = default_points.copy() #copy numpy array and work with its copy\n\n    for _ in range(num_additional_points):\n        # choose random point from existing points\n        random_index = numpy.random.randint(len(points))\n        random_point = points[random_index]\n        x, y = random_point\n\n        # edit offset interval if too close to edge\n        offset_interval = 100\n        if  x < lower_interval_bound + 100 or x > upper_interval_bound-100 or y < lower_interval_bound + 100 or y > upper_interval_bound-100:\n            offset_interval -= close_to_edge_penalty\n\n        # generate offset\n        X_offset = numpy.random.randint(-offset_interval, offset_interval)\n        Y_offset = numpy.random.randint(-offset_interval, offset_interval)\n\n        # set new point\n        new_x = x+X_offset\n        new_y = y+Y_offset\n\n        # append new point to the points\n        points = numpy.append(points, numpy.array([[new_x, new_y]]), axis = 0)\n\n    return points\ndef visualise_clusters(clusters, medoids):\n    # colors to choose from\n    colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\", \"pink\", \"brown\", \"cyan\", \"magenta\", \"lime\", \"indigo\", \"violet\", \"gold\", \"silver\", \"turquoise\", \"maroon\", \"navy\", \"coral\", \"teal\"]\n    plt.figure(figsize=(10, 10))\n\n    # for each cluster, show one point on matplotlib\n    for i in range(len(clusters)):\n        cluster = clusters[i]\n        if len(cluster) > 0:\n            cluster_points = numpy.array(cluster)\n            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i % len(colors)], s=10, label='Cluster {}'.format(i+1))\n\n    # Plot centroids with larger size and different color\n    medoids = numpy.array(medoids)\n    plt.scatter(medoids[:, 0], medoids[:, 1], c='black', s=100, marker='x', label='Centroids', alpha=0.7)\n\n    # aditional matplotlib settings\n    plt.xlim(-5000, 5000)\n    plt.ylim(-5000, 5000)\n    plt.title(\"K-Means Clustering (medoid)\")\n    plt.xlabel(\"X Coordinate\")\n    plt.ylabel(\"Y Coordinate\")\n    plt.legend()\n    plt.grid(False)\n    plt.show()\n\ndef euclidean_distance(point, other_point):\n    # calculate euclidian distance based on formula \n    return numpy.sqrt(numpy.sum(((other_point - point) ** 2)))\n\ndef k_means(points, k, lower_interval_bound, upper_interval_bound):\n    # choose k random medoids \n    medoids = []\n    for _ in range(k):\n        medoids.append([numpy.random.randint(lower_interval_bound, upper_interval_bound), numpy.random.randint(lower_interval_bound, upper_interval_bound)])\n\n    #asign points to the nearest centroid\n    i = 1\n    \n    current_avg = 0\n    previous_avg = 0\n\n    no_update_count = 0\n    average_distances = 0\n    average_distances_count = 0\n    while True:\n        print(f\"Iteration no.: {i}\")\n        i -=- 1\n\n        clusters = [[] for _ in range(k)] #define k clusters\n        distances_to_medoids = []\n\n        # put each point to cluster\n        for point in points:\n            all_distances = [euclidean_distance(point, medoid) for medoid in medoids]\n\n            average_distances += sum(all_distances)\n            average_distances_count += len(all_distances)\n\n            closest_medoid = numpy.argmin(all_distances)\n            distances_to_medoids.append(all_distances[closest_medoid])\n            clusters[closest_medoid].append(point)\n\n        # Calculate the average distance for this iteration\n        if distances_to_medoids:\n            average_distances = sum(distances_to_medoids)\n            average_distances_count = len(distances_to_medoids)\n            current_avg = average_distances / average_distances_count\n            print(f\"Average distance to closest medoid: {current_avg:.2f}\")\n\n        #recalculate medoids\n        new_medoids = []\n        for cluster in clusters:\n            if len(cluster) > 0:\n                cluster_mean = numpy.mean(cluster, axis=0)\n                medoid = min(cluster, key=lambda point: euclidean_distance(point, cluster_mean))\n                new_medoids.append(medoid)\n            else:\n                new_medoids.append(points[numpy.random.randint(len(points))])\n\n        if no_update_count > 0:\n            print(f\"No update for {no_update_count}/3\")\n\n        if abs(previous_avg - current_avg) < 1:\n            no_update_count += 1\n        else:\n            no_update_count = 0  # Reset if assignments have changed\n\n\n        # Stop if there's no update for 3 iterations\n        if no_update_count >= 3:\n            print(f\"    very small update for {no_update_count} iterations, not worth to continue\")\n            break\n\n        # Update centroids for the next iteration\n        medoids = new_medoids\n        previous_avg = current_avg\n\n    return clusters, medoids\n\ndef evaluate_cluster_average_mean(clusters, centroids, k):\n\n    succesful_clusters = 0\n\n    # calculate the sum of distances between each point in the cluster and the centroid of that cluster\n    for i, cluster in enumerate(clusters):\n        if len(cluster) > 0:\n            # Sum up the distances from each point in the cluster to its centroid\n            total_distance = sum(euclidean_distance(point, centroids[i]) for point in cluster)\n            avg_distance = total_distance / len(cluster)\n            if avg_distance <= 500:\n                succesful_clusters += 1\n            print(f\"Average distance for cluster {i + 1}: {avg_distance:.2f}\")\n        else: \n            print(f\"Cluster {i+1} was left empty\")\n\n    print(f\"Number of cluster with average distance from centroid smaller than 500 is {succesful_clusters} from {k} possible\")\n\n    return\n####\n\n \n# !START HERE!\n\n#important variables\nnum_points = 20\nlower_interval_bound = -5000\nupper_interval_bound = 5000\n\nnum_additional_points = 40000\nclose_to_edge_penalty = 10\n\nk = 15 #number of clusters\n\n#inicialize first points\npoints = gen",
        "middle": "erate_start_points(num_p",
        "suffix": "oints, lower_interval_bound, upper_interval_bound) \n# find additional points\nadditional_points = generate_additional_points(points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty) \n\nclusters, medoids = k_means(additional_points, k, lower_interval_bound, upper_interval_bound)\nevaluate_cluster_average_mean(clusters, medoids, k)\nprint(\"buliding visualisation...\")\nvisualise_clusters(clusters, medoids)\n"
    },
    {
        "prefix": "from matplotlib import pyplot as plt\nimport numpy\n\n# !FUNCTIONS!\n\ndef generate_start_points(num_points, lower_interval_bound, upper_interval_bound):\n    points_set = set() # use set so we have unique points\n\n    while len(points_set) < num_points: #untill points_set is not full - based on num_points\n        #generate random point (x,y)\n        x = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1) \n        y = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n\n        points_set.add((x, y)) #add x and y as a tuple into the set\n\n    return numpy.array(list(points_set))  # convert set to numpy array and return\ndef generate_additional_points(default_points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty):\n    points = default_points.copy() #copy numpy array and work with its copy\n\n    for _ in range(num_additional_points):\n        # choose random point from existing points\n        random_index = numpy.random.randint(len(points))\n        random_point = points[random_index]\n        x, y = random_point\n\n        # edit offset interval if too close to edge\n        offset_interval = 100\n        if  x < lower_interval_bound + 100 or x > upper_interval_bound-100 or y < lower_interval_bound + 100 or y > upper_interval_bound-100:\n            offset_interval -= close_to_edge_penalty\n\n        # generate offset\n        X_offset = numpy.random.randint(-offset_interval, offset_interval)\n        Y_offset = numpy.random.randint(-offset_interval, offset_interval)\n\n        # set new point\n        new_x = x+X_offset\n        new_y = y+Y_offset\n\n        # append new point to the points\n        points = numpy.append(points, numpy.array([[new_x, new_y]]), axis = 0)\n\n    return points\ndef visualise_clusters(clusters, medoids):\n    # colors to choose from\n    colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\", \"pink\", \"brown\", \"cyan\", \"magenta\", \"lime\", \"indigo\", \"violet\", \"gold\", \"silver\", \"turquoise\", \"maroon\", \"navy\", \"coral\", \"teal\"]\n    plt.figure(figsize=(10, 10))\n\n    # for each cluster, show one point on matplotlib\n    for i in range(len(clusters)):\n        cluster = clusters[i]\n        if len(cluster) > 0:\n            cluster_points = numpy.array(cluster)\n            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i % len(colors)], s=10, label='Cluster {}'.format(i+1))\n\n    # Plot centroids with larger size and different color\n    medoids = numpy.array(medoids)\n    plt.scatter(medoids[:, 0], medoids[:, 1], c='black', s=100, marker='x', label='Centroids', alpha=0.7)\n\n    # aditional matplotlib settings\n    plt.xlim(-5000, 5000)\n    plt.ylim(-5000, 5000)\n    plt.title(\"K-Means Clustering (medoid)\")\n    plt.xlabel(\"X Coordinate\")\n    plt.ylabel(\"Y Coordinate\")\n    plt.legend()\n    plt.grid(False)\n    plt.show()\n\ndef euclidean_distance(point, other_point):\n    # calculate euclidian distance based on formula \n    return numpy.sqrt(numpy.sum(((other_point - point) ** 2)))\n\ndef k_means(points, k, lower_interval_bound, upper_interval_bound):\n    # choose k random medoids \n    medoids = []\n    for _ in range(k):\n        medoids.append([numpy.random.randint(lower_interval_bound, upper_interval_bound), numpy.random.randint(lower_interval_bound, upper_interval_bound)])\n\n    #asign points to the nearest centroid\n    i = 1\n    \n    current_avg = 0\n    previous_avg = 0\n\n    no_update_count = 0\n    average_distances = 0\n    average_distances_count = 0\n    while True:\n        print(f\"Iteration no.: {i}\")\n        i -=- 1\n\n        clusters = [[] for _ in range(k)] #define k clusters\n        distances_to_medoids = []\n\n        # put each point to cluster\n        for point in points:\n            all_distances = [euclidean_distance(point, medoid) for medoid in medoids]\n\n            average_distances += sum(all_distances)\n            average_distances_count += len(all_distances)\n\n            closest_medoid = numpy.argmin(all_distances)\n            distances_to_medoids.append(all_distances[closest_medoid])\n            clusters[closest_medoid].append(point)\n\n        # Calculate the average distance for this iteration\n        if distances_to_medoids:\n            average_distances = sum(distances_to_medoids)\n            average_distances_count = len(distances_to_medoids)\n            current_avg = average_distances / average_distances_count\n            print(f\"Average distance to closest medoid: {current_avg:.2f}\")\n\n        #recalculate medoids\n        new_medoids = []\n        for cluster in clusters:\n            if len(cluster) > 0:\n                cluster_mean = numpy.mean(cluster, axis=0)\n                medoid = min(cluster, key=lambda point: euclidean_distance(point, cluster_mean))\n                new_medoids.append(medoid)\n            else:\n                new_medoids.append(points[numpy.random.randint(len(points))])\n\n        if no_update_count > 0:\n            print(f\"No update for {no_update_count}/3\")\n\n        if abs(previous_avg - current_avg) < 1:\n            no_update_count += 1\n        else:\n            no_update_count = 0  # Reset if assignments have changed\n\n\n        # Stop if there's no update for 3 iterations\n        if no_update_count >= 3:\n            print(f\"    very small update for {no_update_count} iterations, not worth to continue\")\n            break\n\n        # Update centroids for the next iteration\n        medoids = new_medoids\n        previous_avg = current_avg\n\n    return clusters, medoids\n\ndef evaluate_cluster_average_mean(clusters, centroids, k):\n\n    succesful_clusters = 0\n\n    # calculate the sum of distances between each point in the cluster and the centroid of that cluster\n    for i, cluster in enumerate(clusters):\n        if len(cluster) > 0:\n            # Sum up the distances from each point in the cluster to its centroid\n            total_distance = sum(euclidean_distance(point, centroids[i]) for point in cluster)\n            avg_distance = total_distance / len(cluster)\n            if avg_distance <= 500:\n                succesful_clusters += 1\n            print(f\"Average distance for cluster {i + 1}: {avg_distance:.2f}\")\n        else: \n            print(f\"Cluster {i+1} was left empty\")\n\n    print(f\"Number of cluster with average distance from centroid smaller than 500 is {succesful_clusters} from {k} possible\")\n\n    return\n####\n\n \n# !START HERE!\n\n#important variables\nnum_points = 20\nlower_interval_bound = -5000\nupper_interval_bound = 5000\n\nnum_additional_points = 40000\nclose_to_edge_penalty = 10\n\nk = 15 #number of clusters\n\n#inicialize first points\npoints = generate_start_points(num_points, lower_interval_bound, upper_interval_bound) \n# find additional points\nadditional_points = generate_additional_points(points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty) \n\nclusters, medoids = k_means(additional_points, k, lowe",
        "middle": "r_interval_bound, ",
        "suffix": "upper_interval_bound)\nevaluate_cluster_average_mean(clusters, medoids, k)\nprint(\"buliding visualisation...\")\nvisualise_clusters(clusters, medoids)\n"
    },
    {
        "prefix": "import torch\nimport torch.nn as nn  # all neural network modules\nimport torch.optim as optim  # optimization algo\nfrom torch.utils.data import DataLoader  # easier dataset management, helps create mini batches\nimport torchvision.datasets as datasets  # standard datasets\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt  # For plotting graphs\nfrom sklearn.metrics import confusion_matrix\n\n\nfrom NeuralNwModule import NN  # Your custom neural network module\n\n# Initialize variables for best accuracy tracking\nbest_accuracy = 0\nbest_epoch = 0\n\n# Hyperparameters and constants\nbatch_size = 64\ninput_size = 784  # 28*28\nnum_outputs = 10  # numbers from 0-9\nlearning_rate = 0.001\nnum_epochs = 5\n\n# Load Data\ntrain_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n\ntest_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n\n# Set device\ndevice = torch.device('cpu')\n\n# Initialize neural network\nnetwork = NN(input_size, num_outputs).to(device)\n\n# Loss function\nmisstake = nn.CrossEntropyLoss()\n\n# Choose optimizer\nprint(\"Choose optimizer:\")\nprint(\"1: SGD (Stochastic Gradient Descent)\")\nprint(\"2: SGD with Momentum\")\nprint(\"3: Adam (Adaptive Moment Estimation)\")\nchoice = input(\"Enter your choice (1/2/3): \").strip().lower()\n\nif choice == \"1\" or choice == \"sgd\":\n    optimizer = optim.SGD(network.parameters(), lr=learning_rate)\nelif choice == \"2\" or choice == \"sgdm\":\n    momentum_to_use = float(input(\"Enter the momentum to use (0.9 recommended): \"))\n    optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum_to_use)\nelif choice == \"3\" or choice == \"adam\":\n    optimizer = optim.Adam(network.parameters(), lr=learning_rate)\nelse:\n    raise ValueError(\"Invalid optimizer choice! Please restart and choose a valid option.\")\n\n# Track losses and accuracy\ntrain_losses = []\ntest_losses = []\ntest_accuracies = []\n\n# confusion matrix generation from chatGPT\ndef plot_confusion_matrix(y_true, prediction, classes, epoch=None):\n    # Compute the confusion matrix\n    conf_matrix = confusion_matrix(y_true, prediction)\n\n    # Plot the confusion matrix\n    plt.figure(figsize=(8, 6))\n    plt.imshow(conf_matrix, interpolation=\"nearest\", cmap=\"Blues\")\n    plt.title(f\"Confusion Matrix\" + (f\" (Epoch {epoch + 1})\" if epoch is not None else \"\"))\n    plt.colorbar()\n\n    # Add labels to the axes\n    tick_marks = range(len(classes))\n    plt.xticks(tick_marks, classes)\n    plt.yticks(tick_marks, classes)\n\n    # Add numbers inside the cells\n    for i in range(conf_matrix.shape[0]):\n        for j in range(conf_matrix.shape[1]):\n            plt.text(j, i, format(conf_matrix[i, j], \"d\"), ha=\"center\", va=\"center\", color=\"black\")\n\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    plt.tight_layout()\n    plt.show()\n\n# Function to calculate accuracy and loss\ndef check_accuracy_and_loss(loader, nw, ep=0):\n    global best_accuracy\n    global best_epoch\n\n    num_correct = 0\n    num_samples = 0\n    total_loss = 0\n    nw.eval()\n\n    all_preds = []  # Collect all predictions\n    all_labels = []  # Collect all ground-truth labels\n\n    with torch.no_grad():  # Don't compute gradients\n        for x, y in loader:\n            x = x.to(device) # sent to CPU for computation\n            y = y.to(device)\n            x = x.reshape(x.shape[0], -1) # converts 2d tensor to 1d tensor\n\n            scores = nw(x) # forward pass\n            loss = misstake(scores, y)  # Compute loss using CrossEntropyLoss\n            total_loss += loss.item() # Accumulate loss for the batch\n\n            # evaluate, check predictions and count them\n            _, predicted = scores.max(1)  # Predicted labels\n            num_correct += (predicted == y).sum()\n            num_samples += predicted.size(0)\n\n            if not loader.dataset.train:  # For confusion matrix, collect only test data\n                all_preds.append(predicted.cpu())\n                all_labels.append(y.cpu())\n\n    accuracy = 100 * num_correct / num_samples\n    average_loss = total_loss / len(loader)\n\n    if loader.dataset.train:\n        train_losses.append(average_loss)  # Append t",
        "middle": "raining loss\n        print(f\"",
        "suffix": "Training Accuracy: {accuracy:.2f}%, Loss: {average_loss:.4f}\")\n    else:\n        test_losses.append(average_loss)  # Append test loss\n        test_accuracies.append(accuracy)  # Append test accuracy\n\n        # Update best accuracy\n        if accuracy > best_accuracy:\n            best_accuracy = accuracy\n            best_epoch = ep\n\n        print(f\"Test Accuracy: {accuracy:.2f}%, Loss: {average_loss:.4f}\")\n\n        # Plot confusion matrix\n        all_preds = torch.cat(all_preds).numpy() # list of predictions\n        all_labels = torch.cat(all_labels).numpy() # contains 1D tensors of targets\n        plot_confusion_matrix(all_labels, all_preds, classes=[str(i) for i in range(10)], epoch=ep)\n\n    nw.train()\n\n\n# Train network\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data = data.to(device)\n        targets = target.to(device)\n\n        # Flatten data\n        data = data.reshape(data.shape[0], -1)\n\n        # Forward pass\n        scores = network(data) # Output of the network\n        loss = misstake(scores, targets)\n\n        # Backward pass\n        optimizer.zero_grad()  # Clear previous gradients\n        loss.backward() # compute gradients\n\n        # Gradient descent step - update weights\n        optimizer.step()\n\n    # Check accuracy and loss after each epoch\n    check_accuracy_and_loss(train_loader, network, epoch)\n    check_accuracy_and_loss(test_loader, network, epoch)\n\n# Plotting the Loss and Accuracy graphs\nplt.figure(figsize=(12, 6))\n\n# Subplot 1: Cross-Entropy Loss\nplt.subplot(1, 2, 1)\nplt.plot(range(1, num_epochs + 1), train_losses, label=\"Training Loss\", color=\"blue\")\nplt.plot(range(1, num_epochs + 1), test_losses, label=\"Test Loss\", color=\"red\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Cross-Entropy Loss\")\nplt.title(\"Loss Over Epochs\")\nplt.legend()\n\n# Subplot 2: Test Accuracy\nplt.subplot(1, 2, 2)\nplt.plot(range(1, num_epochs + 1), test_accuracies, label=\"Test Accuracy\", color=\"green\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy (%)\")\nplt.title(\"Test Accuracy Over Epochs\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nBest accuracy on test data: {best_accuracy:.2f}% in epoch {best_epoch+1}\")\n"
    },
    {
        "prefix": "import torch\nimport torch.nn as nn  # all neural network modules\nimport torch.optim as optim  # optimization algo\nfrom torch.utils.data import DataLoader  # easier dataset management, helps create mini batches\nimport torchvision.datasets as datasets  # standard datasets\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt  # For plotting graphs\nfrom sklearn.metrics import confusion_matrix\n\n\nfrom NeuralNwModule import NN  # Your custom neural network module\n\n# Initialize variables for best accuracy tracking\nbest_accuracy = 0\nbest_epoch = 0\n\n# Hyperparameters and constants\nbatch_size = 64\ninput_size = 784  # 28*28\nnum_outputs = 10  # numbers from 0-9\nlearning_rate = 0.001\nnum_epochs = 5\n\n# Load Data\ntrain_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n\ntest_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n\n# Set device\ndevice = torch.device('cpu')\n\n# Initialize neural network\nnetwork = NN(input_size, num_outputs).to(device)\n\n# Loss function\nmisstake = nn.CrossEntropyLoss()\n\n# Choose optimizer\nprint(\"Choose optimizer:\")\nprint(\"1: SGD (Stochastic Gradient Descent)\")\nprint(\"2: SGD with Momentum\")\nprint(\"3: Adam (Adaptive Moment Estimation)\")\nchoice = input(\"Enter your choice (1/2/3): \").strip().lower()\n\nif choice == \"1\" or choice == \"sgd\":\n    optimizer = optim.SGD(network.parameters(), lr=learning_rate)\nelif choice == \"2\" or choice == \"sgdm\":\n    momentum_to_use = float(input(\"Enter the momentum to use (0.9 recommended): \"))\n    optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum_to_use)\nelif choice == \"3\" or choice == \"adam\":\n    optimizer = optim.Adam(network.parameters(), lr=learning_rate)\nelse:\n    raise ValueError(\"Invalid optimizer choice! Please restart and choose a valid option.\")\n\n# Track losses and accuracy\ntrain_losses = []\ntest_losses = []\ntest_accuracies = []\n\n# confusion matrix generation from chatGPT\ndef plot_confusion_matrix(y_true, prediction, classes, epoch=None):\n    # Compute the confusion matrix\n    conf_matrix = confusion_matrix(y_true, prediction)\n\n    # Plot the confusion matrix\n    plt.figure(figsize=(8, 6))\n    plt.imshow(conf_matrix, interpolation=\"nearest\", cmap=\"Blues\")\n    plt.title(f\"Confusion Matrix\" + (f\" (Epoch {epoch + 1})\" if epoch is not None else \"\"))\n    plt.colorbar()\n\n    # Add labels to the axes\n    tick_marks = range(len(classes))\n    plt.xticks(tick_marks, classes)\n    plt.yticks(tick_marks, classes)\n\n    # Add numbers inside the cells\n    for i in range(conf_matrix.shape[0]):\n        for j in range(conf_matrix.shape[1]):\n            plt.text(j, i, format(conf_matrix[i, j], \"d\"), ha=\"center\", va=\"center\", color=\"black\")\n\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    plt.tight_layout()\n    plt.show()\n\n# Function to calculate accuracy and loss\ndef check_accuracy_and_loss(loader, nw, ep=0):\n    global best_accuracy\n    global best_epoch\n\n    num_correct = 0\n    num_samples = 0\n    total_loss = 0\n    nw.eval()\n\n    all_preds = []  # Collect all predictions\n    all_labels = []  # Collect all ground-truth labels\n\n    with torch.no_grad():  # Don't compute gradients\n        for x, y in loader:\n            x = x.to(device) # sent to CPU for computation\n            y = y.to(device)\n            x = x.reshape(x.shape[0], -1) # converts 2d tensor to 1d tensor\n\n            scores = nw(x) # forward pass\n            loss = misstake(scores, y)  # Compute loss using CrossEntropyLoss\n            total_loss += loss.item() # Accumulate loss for the batch\n\n            # evaluate, check predictions and count them\n            _, predicted = scores.max(1)  # Predicted labels\n            num_correct += (predicted == y).sum()\n            num_samples += predicted.size(0)\n\n            if not loader.dataset.train:  # For confusion matrix, collect only test data\n                all_preds.append(predicted.cpu())\n                all_labels.append(y.cpu())\n\n    accuracy = 100 * num_correct / num_samples\n    average_loss = total_loss / len(loader)\n\n    if loader.dataset.train:\n        train_losses.append(average_loss)  # Append training loss\n        print(f\"Training Accuracy: {accuracy:.2f}%, Loss: {average_loss:.4f}\")\n    else:\n        test_losses.append(average_loss)  # Append test loss\n        test_accuracies.append(accuracy)  # Append test accuracy\n\n        # Update best accuracy\n        if accuracy > best_accuracy:\n            best_accuracy = accuracy\n            best_epoch = ep\n\n        print(f\"Test Accuracy: {accuracy:.2f}%, Loss: {average_loss:.4f}\")\n\n        # Plot confusion matrix\n        all_preds = torch.cat(all_preds).numpy() # list of predictions\n        all_labels = torch.cat(all_labels).numpy() # contains 1D tensors of targets\n        plot_confusion_matrix(all_labels, all_preds, classes=[str(i) for i in range(10)], epoch=ep)\n\n    nw.train()\n\n\n# Train network\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data = data.to(device)\n        targets = target.to(device)\n\n        # Flatten data\n        data = data.reshape(data.shape[0], -1)\n\n        # Forward pass\n        scores = network(data) # Output of the network\n        loss = misstake(scores, targets)\n\n        # Backward pass\n        optimizer.zero_grad()  # Clear previous gradients\n        loss.backward() # compute gradients\n\n        # Gradient descent step - update weights\n        optimizer.step()\n\n    # Check accuracy and loss after each epoch\n    check_accuracy_and_loss(train_loader, network, epoch)\n    check_accuracy_and_loss(test_loader, network, epoch)\n\n# Plotting the Loss and Accuracy graphs\nplt.figure(figsize=(12,",
        "middle": " 6))\n\n# Subplot ",
        "suffix": "1: Cross-Entropy Loss\nplt.subplot(1, 2, 1)\nplt.plot(range(1, num_epochs + 1), train_losses, label=\"Training Loss\", color=\"blue\")\nplt.plot(range(1, num_epochs + 1), test_losses, label=\"Test Loss\", color=\"red\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Cross-Entropy Loss\")\nplt.title(\"Loss Over Epochs\")\nplt.legend()\n\n# Subplot 2: Test Accuracy\nplt.subplot(1, 2, 2)\nplt.plot(range(1, num_epochs + 1), test_accuracies, label=\"Test Accuracy\", color=\"green\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy (%)\")\nplt.title(\"Test Accuracy Over Epochs\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nBest accuracy on test data: {best_accuracy:.2f}% in epoch {best_epoch+1}\")\n"
    },
    {
        "prefix": "import torch\nimport torch.nn as nn  # all neural network modules\nimport torch.optim as optim  # optimization algo\nfrom torch.utils.data import DataLoader  # easier dataset management, helps create mini batches\nimport torchvision.datasets as datasets  # standard datasets\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt  # For plotting graphs\nfrom sklearn.metrics import confusion_matrix\n\n\nfrom NeuralNwModule import NN  # Your custom neural network module\n\n# Initialize variables for best accuracy tracking\nbest_accuracy = 0\nbest_epoch = 0\n\n# Hyperparameters and constants\nbatch_size = 64\ninput_size = 784  # 28*28\nnum_outputs = 10  # numbers from 0-9\nlearning_rate = 0.001\nnum_epochs = 5\n\n# Load Data\ntrain_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n\ntest_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n\n# Set device\ndevice = torch.device('cpu')\n\n# Initialize neural network\nnetwork = NN(input_size, num_outputs).to(device)\n\n# Loss function\nmisstake = nn.CrossEntropyLoss()\n\n# Choose optimizer\nprint(\"Choose optimizer:\")\nprint(\"1: SGD (Stochastic Gradient Descent)\")\nprint(\"2: SGD with Momentum\")\nprint(\"3: Adam (Adaptive Moment Estimation)\")\nchoice = input(\"Enter your choice (1/2/3): \").strip().lower()\n\nif choice == \"1\" or choice == \"sgd\":\n    optimizer = optim.SGD(network.parameters(), lr=learning_rate)\nelif choice == \"2\" or choice == \"sgdm\":\n    momentum_to_use = float(input(\"Enter the momentum to use (0.9 recommended): \"))\n    optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum_to_use)\nelif choice == \"3\" or choice == \"adam\":\n    optimizer = optim.Adam(network.parameters(), lr=learning_rate)\nelse:\n    raise ValueError(\"Invalid optimizer choice! Please restart and choose a valid option.\")\n\n# Track losses and accuracy\ntrain_losses = []\ntest_losses = []\ntest_accuracies = []\n\n# confusion matrix generation from chatGPT\ndef plot_confusion_matrix(y_true, prediction, classes, epoch=None):\n    # Compute the confusion matrix\n    conf_matrix = confusion_matrix(y_true, prediction)\n\n    # Plot the confusion matrix\n    plt.figure(figsize=(8, 6))\n    plt.imshow(conf_matrix, interpolation=\"nearest\", cmap=\"Blues\")\n    plt.title(f\"Confusion Matrix\" + (f\" (Epoch {epoch + 1})\" if epoch is not None else \"\"))\n    plt.colorbar()\n\n    # Add labels to the axes\n    tick_marks = range(len(classes))\n    plt.xticks(tick_marks, classes)\n    plt.yticks(tick_marks, classes)\n\n    # Add numbers inside the cells\n    for i in range(conf_matrix.shape[0]):\n        for j in range(conf_matrix.shape[1]):\n            plt.text(j, i, format(conf_matrix[i, j], \"d\"), ha=\"center\", va=\"center\", color=\"black\")\n\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    plt.tight_layout()\n    plt.show()\n\n# Function to calculate accuracy and loss\ndef check_accuracy_and_loss(loader, nw, ep=0):\n    global best_accuracy\n    global best_epoch\n\n    num_correct = 0\n    num_samples = 0\n    total_loss = 0\n    nw.eval()\n\n    all_preds = []  # Collect all predictions\n    all_labels = []  # Collect all ground-truth labels\n\n    with torch.no_grad():  # Don't compute gradients\n        for x, y in loader:\n            x = x.to(device) # sent to CPU for computation\n            y = y.to(device)\n            x = x.reshape(x.shape[0], -1) # converts 2d tensor to 1d tensor\n\n            scores = nw(x) # forward pass\n            loss = misstake(scores, y)  # Compute loss using CrossEntropyLoss\n            total_loss += loss.item() # Accumulate loss for the batch\n\n            # evaluate, check predictions and count them\n            _, predicted = scores.max(1)  # Predicted labels\n            num_correct += (predicted == y).sum()\n            num_samples += predicted.size(0)\n\n            if not loader.dataset.train:  # For confusion matrix, collect only test data\n                all_preds.append(predicted.cpu())\n                all_labels.append(y.cpu())\n\n    accuracy = 100 * num_correct / num_samples\n    average_loss = total_loss / len(loader)\n\n    if loader.dataset.train:\n        train_losses.append(average_loss)  # Append training loss\n        print(f\"Training Accuracy: {accuracy:.2f}%, Loss: {average_loss:.4f}\")\n    else:\n        test_losses.append(average_loss)  # Append test loss\n        test_accuracies.append(accuracy)  # Append test accuracy\n\n        # Update best accuracy\n        if accuracy > best_accuracy:\n            best_accuracy = accuracy\n            best_epoch = ep\n\n        print(f\"Test Accuracy: {accuracy:.2f}%, Loss: {average_loss:.4f}\")\n\n        # Plot confusion matrix\n        all_preds = torch.cat(all_preds).numpy() # list of predictions\n        all_labels = torch.cat(all_labels).numpy() # contains 1D tensors of targets\n        plot_confusion_matrix(all_labels, all_preds, classes=[str(i) for i in range(10)], epoch=ep)\n\n    nw.train()\n\n\n# Train network\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data = data.to(device)\n        targets = target.to(device)\n\n        # Flatten data\n        data = data.reshape(data.shape[0], -1)\n\n        # Forward pass\n        scores = network(data) # Output of the n",
        "middle": "etwork\n        loss = mis",
        "suffix": "stake(scores, targets)\n\n        # Backward pass\n        optimizer.zero_grad()  # Clear previous gradients\n        loss.backward() # compute gradients\n\n        # Gradient descent step - update weights\n        optimizer.step()\n\n    # Check accuracy and loss after each epoch\n    check_accuracy_and_loss(train_loader, network, epoch)\n    check_accuracy_and_loss(test_loader, network, epoch)\n\n# Plotting the Loss and Accuracy graphs\nplt.figure(figsize=(12, 6))\n\n# Subplot 1: Cross-Entropy Loss\nplt.subplot(1, 2, 1)\nplt.plot(range(1, num_epochs + 1), train_losses, label=\"Training Loss\", color=\"blue\")\nplt.plot(range(1, num_epochs + 1), test_losses, label=\"Test Loss\", color=\"red\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Cross-Entropy Loss\")\nplt.title(\"Loss Over Epochs\")\nplt.legend()\n\n# Subplot 2: Test Accuracy\nplt.subplot(1, 2, 2)\nplt.plot(range(1, num_epochs + 1), test_accuracies, label=\"Test Accuracy\", color=\"green\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy (%)\")\nplt.title(\"Test Accuracy Over Epochs\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nBest accuracy on test data: {best_accuracy:.2f}% in epoch {best_epoch+1}\")\n"
    },
    {
        "prefix": "import torch\nimport torch.nn as nn  # all neural network modules\nimport torch.optim as optim  # optimization algo\nfrom torch.utils.data import DataLoader  # easier dataset management, helps create mini batches\nimport torchvision.datasets as datasets  # standard datasets\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt  # For plotting graphs\nfrom sklearn.metrics import confusion_matrix\n\n\nfrom NeuralNwModule import NN  # Your custom neural network module\n\n# Initialize variables for best accuracy tracking\nbest_accuracy = 0\nbest_epoch = 0\n\n# Hyperparameters and constants\nbatch_size = 64\ninput_size = 784  # 28*28\nnum_outputs = 10  # numbers from 0-9\nlearning_rate = 0.001\nnum_epochs = 5\n\n# Load Data\ntrain_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n\ntest_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n\n# Set device\ndevice = torch.device('cpu')\n\n# Initialize neural network\nnetwork = NN(input_size, num_outputs).to(device)\n\n# Loss function\nmisstake = nn.CrossEntropyLoss()\n\n# Choose optimizer\nprint(\"Choose optimizer:\")\nprint(\"1: SGD (Stochastic Gradient Descent)\")\nprint(\"2: SGD with Momentum\")\nprint(\"3: Adam (Adaptive Moment Estimation)\")\nchoice = input(\"Enter your choice (1/2/3): \").strip().lower()\n\nif choice == \"1\" or choice == \"sgd\":\n    optimizer = optim.SGD(network.parameters(), lr=learning_rate)\nelif choice == \"2\" or choice == \"sgdm\":\n    momentum_to_use = float(input(\"Enter the momentum to use (0.9 recommended): \"))\n    optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum_to_use)\nelif choice == \"3\" or choice == \"adam\":\n    optimizer = optim.Adam(network.parameters(), lr=learning_rate)\nelse:\n    raise ValueError(\"Invalid optimizer choice! Please restart and choose a valid option.\")\n\n# Track losses and accuracy\ntrain_losses = []\ntest_losses = []\ntest_accuracies = []\n\n# confusion matrix generation from chatGPT\ndef plot_confusion_matrix(y_true, prediction, classes, epoch=None):\n    # Compute the confusion matrix\n    conf_matrix = confusion_matrix(y_true, prediction)\n\n    # Plot the confusion matrix\n    plt.figure(figsize=(8, 6))\n    plt.imshow(conf_matrix, interpolation=\"nearest\", cmap=\"Blues\")\n    plt.title(f\"Confusion Matrix\" + (f\" (Epoch {epoch + 1})\" if epoch is not None else \"\"))\n    plt.colorbar()\n\n    # Add labels to the axes\n    tick_marks = range(len(classes))\n    plt.xticks(tick_marks, classes)\n    plt.yticks(tick_marks, classes)\n\n    # Add numbers inside the cells\n    for i in range(conf_matrix.shape[0]):\n        for j in range(conf_matrix.shape[1]):\n            plt.text(j, i, format(conf_matrix[i, j], \"d\"), ha=\"center\", va=\"center\", color=\"black\")\n\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    plt.tight_layout()\n    plt.show()\n\n# Function to calculate accuracy and loss\ndef check_accuracy_and_loss(loader, nw, ep=0):\n    global best_accuracy\n    global best_epoch\n\n    num_correct = 0\n    num_samples = 0\n    total_loss = 0\n    nw.eval()\n\n    all_preds = []  # Collect all predictions\n    all_labels = []  # Collect all ground-truth labels\n\n    with torch.no_grad():  # Don't compute gradients\n        for x, y in loader:\n            x = x.to(device) # sent to CPU for computation\n            y = y.to(device)\n            x = x.reshape(x.shape[0], -1) # converts 2d tensor to 1d tensor\n\n            scores = nw(x) # forward pass\n            loss = misstake(scores, y)  # Compute loss using CrossEntropyLoss\n            total_loss += loss.item() # Accumulate loss for the batch\n\n            # evaluate, check predictions and count them\n            _, predicted = scores.max(1)  # Predicted labels\n            num_correct += (predicted == y).sum()\n            num_samples += predicted.size(0)\n\n            if not loader.dataset.train:  # For confusion matrix, collect only test data\n                all_preds.append(predicted.cpu())\n                all_labels.append(y.cpu())\n\n    accuracy = 100 * num_correct / num_samples\n    average_loss = total_loss / len(loader)\n\n    if loader.dataset.train:\n        train_losses.append(average_loss)  # Append training loss\n        print(f\"Training Accuracy: {accuracy:.2f}%, Loss: {average_loss:.4f}\")\n    else:\n        test_losses.append(average_loss)  # Append test loss\n        test_accuracies.append(accuracy)  # Append test accuracy\n\n        # Update best accuracy\n        if accuracy > best_accuracy:\n            best_accuracy = accuracy\n            b",
        "middle": "est_epoch = ep\n\n        print",
        "suffix": "(f\"Test Accuracy: {accuracy:.2f}%, Loss: {average_loss:.4f}\")\n\n        # Plot confusion matrix\n        all_preds = torch.cat(all_preds).numpy() # list of predictions\n        all_labels = torch.cat(all_labels).numpy() # contains 1D tensors of targets\n        plot_confusion_matrix(all_labels, all_preds, classes=[str(i) for i in range(10)], epoch=ep)\n\n    nw.train()\n\n\n# Train network\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data = data.to(device)\n        targets = target.to(device)\n\n        # Flatten data\n        data = data.reshape(data.shape[0], -1)\n\n        # Forward pass\n        scores = network(data) # Output of the network\n        loss = misstake(scores, targets)\n\n        # Backward pass\n        optimizer.zero_grad()  # Clear previous gradients\n        loss.backward() # compute gradients\n\n        # Gradient descent step - update weights\n        optimizer.step()\n\n    # Check accuracy and loss after each epoch\n    check_accuracy_and_loss(train_loader, network, epoch)\n    check_accuracy_and_loss(test_loader, network, epoch)\n\n# Plotting the Loss and Accuracy graphs\nplt.figure(figsize=(12, 6))\n\n# Subplot 1: Cross-Entropy Loss\nplt.subplot(1, 2, 1)\nplt.plot(range(1, num_epochs + 1), train_losses, label=\"Training Loss\", color=\"blue\")\nplt.plot(range(1, num_epochs + 1), test_losses, label=\"Test Loss\", color=\"red\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Cross-Entropy Loss\")\nplt.title(\"Loss Over Epochs\")\nplt.legend()\n\n# Subplot 2: Test Accuracy\nplt.subplot(1, 2, 2)\nplt.plot(range(1, num_epochs + 1), test_accuracies, label=\"Test Accuracy\", color=\"green\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy (%)\")\nplt.title(\"Test Accuracy Over Epochs\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nBest accuracy on test data: {best_accuracy:.2f}% in epoch {best_epoch+1}\")\n"
    },
    {
        "prefix": "from matplotlib import pyplot as plt\nimport numpy\n\n# !FUNCTIONS!\n\ndef generate_start_points(num_points, lower_interval_bound, upper_interval_bound):\n    points_set = set() #use set so we have unique points\n\n    while len(points_set) < num_points: #untill points_set is not full - based on num_points\n        #generate random point (x,y)\n        x = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n        y = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n\n        points_set.add((x, y)) #add x and y as a tuple into the set\n\n    return numpy.array(list(points_set)) # convert set to numpy array and return\ndef generate_additional_points(default_points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty):\n    points = default_points.copy() #copy numpy array and work with its copy\n\n    for _ in range(num_additional_points):\n        # choose random point from existing points\n        random_index = numpy.random.randint(len(points)) \n        random_point = points[random_index]\n        x, y = random_point\n\n        # edit offset interval if too close to edge\n        offset_interval = 100\n        if  x < lower_interval_bound + 100 or x > upper_interval_bound-100 or y < lower_interval_bound + 100 or y > upper_interval_bound-100:\n            offset_interval -= close_to_edge_penalty\n\n        # generate offset\n        X_offset = numpy.random.randint(-offset_interval, offset_interval)\n        Y_offset = numpy.random.randint(-offset_interval, offset_interval)\n\n        # set new point\n        new_x = x+X_offset\n        new_y = y+Y_offset\n\n        points = numpy.append(points, numpy.array([[new_x, new_y]]), axis = 0) # append new point to the points\n    return points\n\ndef visualise_clusters(clusters, medoids):\n    # colors to choose from\n    colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\", \"pink\", \"brown\", \"cyan\", \"magenta\", \"lime\", \"indigo\", \"violet\", \"gold\", \"silver\", \"turquoise\", \"maroon\", \"navy\", \"coral\", \"teal\"]\n    plt.figure(figsize=(10, 10))\n\n    # for each cluster, show one point on matplotlib\n    for i in range(len(clusters)):\n        cluster = clusters[i]\n        if len(cluster) > 0:\n            cluster_points = numpy.array(cluster)\n            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i % len(colors)], s=10, label='Cluster {}'.format(i+1))\n\n    # plot centroids with larger size and different color\n    medoids = numpy.array(medoids)\n    plt.scatter(medoids[:, 0], medoids[:, 1], c='black', s=100, marker='x', label='Centroids', alpha=0.7)\n\n    # aditional matplotlib settings\n    plt.xlim(-5000, 5000)\n    plt.ylim(-5000, 5000)\n    plt.title(\"Divisive CLustering (centroid - K-Me",
        "middle": "ans)\")\n    plt.xlabel(\"",
        "suffix": "X Coordinate\")\n    plt.ylabel(\"Y Coordinate\")\n    plt.legend()\n    plt.grid(False)\n    plt.show()\n\ndef euclidean_distance(point, other_point):\n    # calculate euclidian distance based on formula \n    return numpy.sqrt(numpy.sum(((other_point - point) ** 2)))\n\ndef evaluate_cluster_average_mean(clusters, centroids, k, print_logs):\n    ## if print_logs is False -> this function is used for calculation during divisive clustering\n    ## else if print_logs is True -> this function is used in the end of code to evaluare average mean\n\n    succesful_clusters = 0\n\n    # calculate the sum of distances between each point in the cluster and the centroid of that cluster\n    for i, cluster in enumerate(clusters):\n        if len(cluster) > 0:\n            # Sum up the distances from each point in the cluster to its centroid\n            total_distance = sum(euclidean_distance(point, centroids[i]) for point in cluster)\n            avg_distance = total_distance / len(cluster)\n            if avg_distance <= 500:\n                succesful_clusters += 1\n            if print_logs:\n                print(f\"Average distance for cluster {i + 1}: {avg_distance:.2f}\")\n        else: \n            if print_logs:\n                print(f\"Cluster {i+1} was left empty\")\n\n    if print_logs:\n        print(f\"Number of cluster with average distance from centroid smaller than 500 is {succesful_clusters} from {k} possible\")\n\n    return succesful_clusters, k\n\ndef k_means(points, k, lower_interval_bound, upper_interval_bound):\n    # choose k random centroids\n    centroids = points[numpy.random.choice(points.shape[0], k, replace=False)]\n\n    #asign points to the nearest centroid\n    i = 1\n    \n    current_avg = 0\n    previous_avg = 0\n\n    no_update_count = 0\n    average_distances = 0\n    average_distances_count = 0\n    while True:\n        print(f\"Iteration no.: {i}\")\n        i -=- 1\n\n        clusters = [[] for _ in range(k)] #define k clusters\n        distances_to_points = []\n        max_distances = [0] * k\n\n        # put each point to cluster\n        for point in points:\n            all_distances = [euclidean_distance(point, centroid) for centroid in centroids]\n\n            average_distances += sum(all_distances)\n            average_distances_count += len(all_distances)\n\n            closest_centroid = numpy.argmin(all_distances)\n            distances_to_points.append(all_distances[closest_centroid]) \n            clusters[closest_centroid].append(point)\n\n            # Update max distance for this centroid\n            max_distances[closest_centroid] = max(max_distances[closest_centroid], all_distances[closest_centroid])\n\n        #recalculate centroids\n        new_centroids = []\n        for cluster in clusters:\n            if len(cluster) > 0:  # Only calculate the mean for non-empty clusters\n                new_centroids.append(numpy.mean(cluster, axis=0))\n            else:\n                # print(\"bububu\")\n                new_centroids.append([numpy.random.randint(lower_interval_bound, upper_interval_bound), numpy.random.randint(lower_interval_bound, upper_interval_bound)])\n\n        if abs(previous_avg - current_avg) < 1: \n            no_update_count += 1\n        else:\n            no_update_count = 0  # Reset if assignments changed\n\n\n        # Stop if there's no update for 3 iterations\n        if no_update_count >= 3:\n            break\n\n        # Update centroids for the next iteration\n        centroids = new_centroids\n        previous_avg = current_avg\n\n    return clusters, centroids\n####\n\ndef divisive_clustering(points, lower_interval_bound, upper_interval_bound):\n\n    clusters = [points] # all points are one cluster at first\n    centroids = [numpy.mean(points, axis=0)]\n\n    while True:\n        # Evaluate the current clusters\n        successful_clusters, total_clusters = evaluate_cluster_average_mean(clusters, centroids, len(clusters), False)\n        \n        # if each clusters average mean is less than 500, stop the clustering \n        if successful_clusters == total_clusters:\n            print(\"All clusters have average less than 500.\")\n            break\n\n        # find the biggest cluster by average distance to its centroid\n        max_distance_cluster_index = -1\n        max_distance_avg = -1\n        for i, cluster in enumerate(clusters):\n            if len(cluster) > 0:\n                centroid = centroids[i]\n                avg_distance = sum(euclidean_distance(point, centroid) for point in cluster) / len(cluster)\n                if avg_distance > max_distance_avg:\n                    max_distance_avg = avg_distance\n                    max_distance_cluster_index = i\n\n        # split the largest cluster if its average distance is above the threshold\n        if max_distance_avg > 500:\n            largest_cluster = clusters.pop(max_distance_cluster_index)\n            centroids.pop(max_distance_cluster_index)  # Remove the centroid of the largest cluster\n\n            # split the biggest cluster with k_means with centroid\n            sub_clusters, sub_medoids = k_means(numpy.array(largest_cluster), 2, lower_interval_bound, upper_interval_bound)\n\n            # add the split clusters back to the main list of clusters\n            clusters.extend(sub_clusters)\n            centroids.extend(sub_medoids)\n        else:\n            break  # stop if all clusters are below the threshold\n\n\n\n\n    return clusters, centroids\n \n\n# !START HERE!\n\n# important variables\nnum_points = 20\nlower_interval_bound = -5000\nupper_interval_bound = 5000\n\nnum_additional_points = 40000\nclose_to_edge_penalty = 10\n\n# inicialize first points\npoints = generate_start_points(num_points, lower_interval_bound, upper_interval_bound) \n# find additional points\nadditional_points = generate_additional_points(points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty) \n\n# start divisive clustering\nclusters, centroids = divisive_clustering(additional_points, lower_interval_bound, upper_interval_bound)\n# print the average mean \nevaluate_cluster_average_mean(clusters, centroids, len(centroids), True)\nprint(\"buliding visualisation...\")\n# visualise clustering\nvisualise_clusters(clusters, centroids)\n"
    },
    {
        "prefix": "from matplotlib import pyplot as plt\nimport numpy\n\n# !FUNCTIONS!\n\ndef generate_start_points(num_points, lower_interval_bound, upper_interval_bound):\n    points_set = set() #use set so we have unique points\n\n    while len(points_set) < num_points: #untill points_set is not full - based on num_points\n        #generate random point (x,y)\n        x = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n        y = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n\n        points_set.add((x, y)) #add x and y as a tuple into the set\n\n    return numpy.array(list(points_set)) # convert set to numpy array and return\ndef generate_additional_points(default_points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty):\n    points = default_points.copy() #copy numpy array and work with its copy\n\n    for _ in range(num_additional_points):\n        # choose random point from existing points\n        random_index = numpy.random.randint(len(points)) \n        random_point = points[random_index]\n        x, y = random_point\n\n        # edit offset interval if too close to edge\n        offset_interval = 100\n        if  x < lower_interval_bound + 100 or x > upper_interval_bound-100 or y < lower_interval_bound + 100 or y > upper_interval_bound-100:\n            offset_interval -= close_to_edge_penalty\n\n        # generate offset\n        X_offset = numpy.random.randint(-offset_interval, offset_interval)\n        Y_offset = numpy.random.randint(-offset_interval, offset_interval)\n\n        # set new point\n        new_x = x+X_offset\n        new_y = y+Y_offset\n\n        points = numpy.append(points, numpy.array([[new_x, new_y]]), axis = 0) # append new point to the points\n    return points\n\ndef visualise_clusters(clusters, medoids):\n    # colors to choose from\n    colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\", \"pink\", \"brown\", \"cyan\", \"magenta\", \"lime\", \"indigo\", \"violet\", \"gold\", \"silver\", \"turquoise\", \"maroon\", \"navy\", \"coral\", \"teal\"]\n    plt.figure(figsize=(10, 10))\n\n    # for each cluster, show one point on matplotlib\n    for i in range(len(clusters)):\n        cluster = clusters[i]\n        if len(cluster) > 0:\n            cluster_points = numpy.array(cluster)\n            plt.scatter(cluster_p",
        "middle": "oints[:, 0], cl",
        "suffix": "uster_points[:, 1], c=colors[i % len(colors)], s=10, label='Cluster {}'.format(i+1))\n\n    # plot centroids with larger size and different color\n    medoids = numpy.array(medoids)\n    plt.scatter(medoids[:, 0], medoids[:, 1], c='black', s=100, marker='x', label='Centroids', alpha=0.7)\n\n    # aditional matplotlib settings\n    plt.xlim(-5000, 5000)\n    plt.ylim(-5000, 5000)\n    plt.title(\"Divisive CLustering (centroid - K-Means)\")\n    plt.xlabel(\"X Coordinate\")\n    plt.ylabel(\"Y Coordinate\")\n    plt.legend()\n    plt.grid(False)\n    plt.show()\n\ndef euclidean_distance(point, other_point):\n    # calculate euclidian distance based on formula \n    return numpy.sqrt(numpy.sum(((other_point - point) ** 2)))\n\ndef evaluate_cluster_average_mean(clusters, centroids, k, print_logs):\n    ## if print_logs is False -> this function is used for calculation during divisive clustering\n    ## else if print_logs is True -> this function is used in the end of code to evaluare average mean\n\n    succesful_clusters = 0\n\n    # calculate the sum of distances between each point in the cluster and the centroid of that cluster\n    for i, cluster in enumerate(clusters):\n        if len(cluster) > 0:\n            # Sum up the distances from each point in the cluster to its centroid\n            total_distance = sum(euclidean_distance(point, centroids[i]) for point in cluster)\n            avg_distance = total_distance / len(cluster)\n            if avg_distance <= 500:\n                succesful_clusters += 1\n            if print_logs:\n                print(f\"Average distance for cluster {i + 1}: {avg_distance:.2f}\")\n        else: \n            if print_logs:\n                print(f\"Cluster {i+1} was left empty\")\n\n    if print_logs:\n        print(f\"Number of cluster with average distance from centroid smaller than 500 is {succesful_clusters} from {k} possible\")\n\n    return succesful_clusters, k\n\ndef k_means(points, k, lower_interval_bound, upper_interval_bound):\n    # choose k random centroids\n    centroids = points[numpy.random.choice(points.shape[0], k, replace=False)]\n\n    #asign points to the nearest centroid\n    i = 1\n    \n    current_avg = 0\n    previous_avg = 0\n\n    no_update_count = 0\n    average_distances = 0\n    average_distances_count = 0\n    while True:\n        print(f\"Iteration no.: {i}\")\n        i -=- 1\n\n        clusters = [[] for _ in range(k)] #define k clusters\n        distances_to_points = []\n        max_distances = [0] * k\n\n        # put each point to cluster\n        for point in points:\n            all_distances = [euclidean_distance(point, centroid) for centroid in centroids]\n\n            average_distances += sum(all_distances)\n            average_distances_count += len(all_distances)\n\n            closest_centroid = numpy.argmin(all_distances)\n            distances_to_points.append(all_distances[closest_centroid]) \n            clusters[closest_centroid].append(point)\n\n            # Update max distance for this centroid\n            max_distances[closest_centroid] = max(max_distances[closest_centroid], all_distances[closest_centroid])\n\n        #recalculate centroids\n        new_centroids = []\n        for cluster in clusters:\n            if len(cluster) > 0:  # Only calculate the mean for non-empty clusters\n                new_centroids.append(numpy.mean(cluster, axis=0))\n            else:\n                # print(\"bububu\")\n                new_centroids.append([numpy.random.randint(lower_interval_bound, upper_interval_bound), numpy.random.randint(lower_interval_bound, upper_interval_bound)])\n\n        if abs(previous_avg - current_avg) < 1: \n            no_update_count += 1\n        else:\n            no_update_count = 0  # Reset if assignments changed\n\n\n        # Stop if there's no update for 3 iterations\n        if no_update_count >= 3:\n            break\n\n        # Update centroids for the next iteration\n        centroids = new_centroids\n        previous_avg = current_avg\n\n    return clusters, centroids\n####\n\ndef divisive_clustering(points, lower_interval_bound, upper_interval_bound):\n\n    clusters = [points] # all points are one cluster at first\n    centroids = [numpy.mean(points, axis=0)]\n\n    while True:\n        # Evaluate the current clusters\n        successful_clusters, total_clusters = evaluate_cluster_average_mean(clusters, centroids, len(clusters), False)\n        \n        # if each clusters average mean is less than 500, stop the clustering \n        if successful_clusters == total_clusters:\n            print(\"All clusters have average less than 500.\")\n            break\n\n        # find the biggest cluster by average distance to its centroid\n        max_distance_cluster_index = -1\n        max_distance_avg = -1\n        for i, cluster in enumerate(clusters):\n            if len(cluster) > 0:\n                centroid = centroids[i]\n                avg_distance = sum(euclidean_distance(point, centroid) for point in cluster) / len(cluster)\n                if avg_distance > max_distance_avg:\n                    max_distance_avg = avg_distance\n                    max_distance_cluster_index = i\n\n        # split the largest cluster if its average distance is above the threshold\n        if max_distance_avg > 500:\n            largest_cluster = clusters.pop(max_distance_cluster_index)\n            centroids.pop(max_distance_cluster_index)  # Remove the centroid of the largest cluster\n\n            # split the biggest cluster with k_means with centroid\n            sub_clusters, sub_medoids = k_means(numpy.array(largest_cluster), 2, lower_interval_bound, upper_interval_bound)\n\n            # add the split clusters back to the main list of clusters\n            clusters.extend(sub_clusters)\n            centroids.extend(sub_medoids)\n        else:\n            break  # stop if all clusters are below the threshold\n\n\n\n\n    return clusters, centroids\n \n\n# !START HERE!\n\n# important variables\nnum_points = 20\nlower_interval_bound = -5000\nupper_interval_bound = 5000\n\nnum_additional_points = 40000\nclose_to_edge_penalty = 10\n\n# inicialize first points\npoints = generate_start_points(num_points, lower_interval_bound, upper_interval_bound) \n# find additional points\nadditional_points = generate_additional_points(points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty) \n\n# start divisive clustering\nclusters, centroids = divisive_clustering(additional_points, lower_interval_bound, upper_interval_bound)\n# print the average mean \nevaluate_cluster_average_mean(clusters, centroids, len(centroids), True)\nprint(\"buliding visualisation...\")\n# visualise clustering\nvisualise_clusters(clusters, centroids)\n"
    },
    {
        "prefix": "from matplotlib import pyplot as plt\nimport numpy\n\n# !FUNCTIONS!\n\ndef generate_start_points(num_points, lower_interval_bound, upper_interval_bound):\n    points_set = set() #use set so we have unique points\n\n    while len(points_set) < num_points: #untill points_set is not full - based on num_points\n        #generate random point (x,y)\n        x = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n        y = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n\n        points_set.add((x, y)) #add x and y as a tuple into the set\n\n    return numpy.array(list(points_set)) # convert set to numpy array and return\ndef generate_additional_points(default_points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty):\n    points = default_points.copy() #copy numpy array and work with its copy\n\n    for _ in range(num_additional_points):\n        # choose random point from existing points\n        random_index = numpy.random.randint(len(points)) \n        random_point = points[random_index]\n        x, y = random_point\n\n        # edit offset interval if too close to edge\n        offset_interval = 100\n        if  x < lower_interval_bound + 100 or x > upper_interval_bound-100 or y < lower_interval_bound + 100 or y > upper_interval_bound-100:\n            offset_interval -= close_to_edge_penalty\n\n        # generate offset\n        X_offset = numpy.random.randint(-offset_interval, offset_interval)\n        Y_offset = numpy.random.randint(-offset_interval, offset_interval)\n\n        # set new point\n        new_x = x+X_offset\n        new_y = y+Y_offset\n\n        points = numpy.append(points, numpy.array([[new_x, new_y]]), axis = 0) # append new point to the points\n    return points\n\ndef visualise_clusters(clusters, medoids):\n    # colors to choose from\n    colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\", \"pink\", \"brown\", \"cyan\", \"magenta\", \"lime\", \"indigo\", \"violet\", \"gold\", \"silver\", \"turquoise\", \"maroon\", \"navy\", \"coral\", \"teal\"]\n    plt.figure(figsize=(10, 10))\n\n    # for each cluster, show one point on matplotlib\n    for i in range(len(clusters)):\n        cluster = clusters[i]\n        if len(cluster) > 0:\n            cluster_points = numpy.array(cluster)\n            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i % len(colors)], s=10, label='Cluster {}'.format(i+1))\n\n    # plot centroids with larger size and different color\n    medoids = numpy.array(medoids)\n    plt.scatter(medoids[:, 0], medoids[:, 1], c='black', s=100, marke",
        "middle": "r='x', label='Centroid",
        "suffix": "s', alpha=0.7)\n\n    # aditional matplotlib settings\n    plt.xlim(-5000, 5000)\n    plt.ylim(-5000, 5000)\n    plt.title(\"Divisive CLustering (centroid - K-Means)\")\n    plt.xlabel(\"X Coordinate\")\n    plt.ylabel(\"Y Coordinate\")\n    plt.legend()\n    plt.grid(False)\n    plt.show()\n\ndef euclidean_distance(point, other_point):\n    # calculate euclidian distance based on formula \n    return numpy.sqrt(numpy.sum(((other_point - point) ** 2)))\n\ndef evaluate_cluster_average_mean(clusters, centroids, k, print_logs):\n    ## if print_logs is False -> this function is used for calculation during divisive clustering\n    ## else if print_logs is True -> this function is used in the end of code to evaluare average mean\n\n    succesful_clusters = 0\n\n    # calculate the sum of distances between each point in the cluster and the centroid of that cluster\n    for i, cluster in enumerate(clusters):\n        if len(cluster) > 0:\n            # Sum up the distances from each point in the cluster to its centroid\n            total_distance = sum(euclidean_distance(point, centroids[i]) for point in cluster)\n            avg_distance = total_distance / len(cluster)\n            if avg_distance <= 500:\n                succesful_clusters += 1\n            if print_logs:\n                print(f\"Average distance for cluster {i + 1}: {avg_distance:.2f}\")\n        else: \n            if print_logs:\n                print(f\"Cluster {i+1} was left empty\")\n\n    if print_logs:\n        print(f\"Number of cluster with average distance from centroid smaller than 500 is {succesful_clusters} from {k} possible\")\n\n    return succesful_clusters, k\n\ndef k_means(points, k, lower_interval_bound, upper_interval_bound):\n    # choose k random centroids\n    centroids = points[numpy.random.choice(points.shape[0], k, replace=False)]\n\n    #asign points to the nearest centroid\n    i = 1\n    \n    current_avg = 0\n    previous_avg = 0\n\n    no_update_count = 0\n    average_distances = 0\n    average_distances_count = 0\n    while True:\n        print(f\"Iteration no.: {i}\")\n        i -=- 1\n\n        clusters = [[] for _ in range(k)] #define k clusters\n        distances_to_points = []\n        max_distances = [0] * k\n\n        # put each point to cluster\n        for point in points:\n            all_distances = [euclidean_distance(point, centroid) for centroid in centroids]\n\n            average_distances += sum(all_distances)\n            average_distances_count += len(all_distances)\n\n            closest_centroid = numpy.argmin(all_distances)\n            distances_to_points.append(all_distances[closest_centroid]) \n            clusters[closest_centroid].append(point)\n\n            # Update max distance for this centroid\n            max_distances[closest_centroid] = max(max_distances[closest_centroid], all_distances[closest_centroid])\n\n        #recalculate centroids\n        new_centroids = []\n        for cluster in clusters:\n            if len(cluster) > 0:  # Only calculate the mean for non-empty clusters\n                new_centroids.append(numpy.mean(cluster, axis=0))\n            else:\n                # print(\"bububu\")\n                new_centroids.append([numpy.random.randint(lower_interval_bound, upper_interval_bound), numpy.random.randint(lower_interval_bound, upper_interval_bound)])\n\n        if abs(previous_avg - current_avg) < 1: \n            no_update_count += 1\n        else:\n            no_update_count = 0  # Reset if assignments changed\n\n\n        # Stop if there's no update for 3 iterations\n        if no_update_count >= 3:\n            break\n\n        # Update centroids for the next iteration\n        centroids = new_centroids\n        previous_avg = current_avg\n\n    return clusters, centroids\n####\n\ndef divisive_clustering(points, lower_interval_bound, upper_interval_bound):\n\n    clusters = [points] # all points are one cluster at first\n    centroids = [numpy.mean(points, axis=0)]\n\n    while True:\n        # Evaluate the current clusters\n        successful_clusters, total_clusters = evaluate_cluster_average_mean(clusters, centroids, len(clusters), False)\n        \n        # if each clusters average mean is less than 500, stop the clustering \n        if successful_clusters == total_clusters:\n            print(\"All clusters have average less than 500.\")\n            break\n\n        # find the biggest cluster by average distance to its centroid\n        max_distance_cluster_index = -1\n        max_distance_avg = -1\n        for i, cluster in enumerate(clusters):\n            if len(cluster) > 0:\n                centroid = centroids[i]\n                avg_distance = sum(euclidean_distance(point, centroid) for point in cluster) / len(cluster)\n                if avg_distance > max_distance_avg:\n                    max_distance_avg = avg_distance\n                    max_distance_cluster_index = i\n\n        # split the largest cluster if its average distance is above the threshold\n        if max_distance_avg > 500:\n            largest_cluster = clusters.pop(max_distance_cluster_index)\n            centroids.pop(max_distance_cluster_index)  # Remove the centroid of the largest cluster\n\n            # split the biggest cluster with k_means with centroid\n            sub_clusters, sub_medoids = k_means(numpy.array(largest_cluster), 2, lower_interval_bound, upper_interval_bound)\n\n            # add the split clusters back to the main list of clusters\n            clusters.extend(sub_clusters)\n            centroids.extend(sub_medoids)\n        else:\n            break  # stop if all clusters are below the threshold\n\n\n\n\n    return clusters, centroids\n \n\n# !START HERE!\n\n# important variables\nnum_points = 20\nlower_interval_bound = -5000\nupper_interval_bound = 5000\n\nnum_additional_points = 40000\nclose_to_edge_penalty = 10\n\n# inicialize first points\npoints = generate_start_points(num_points, lower_interval_bound, upper_interval_bound) \n# find additional points\nadditional_points = generate_additional_points(points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty) \n\n# start divisive clustering\nclusters, centroids = divisive_clustering(additional_points, lower_interval_bound, upper_interval_bound)\n# print the average mean \nevaluate_cluster_average_mean(clusters, centroids, len(centroids), True)\nprint(\"buliding visualisation...\")\n# visualise clustering\nvisualise_clusters(clusters, centroids)\n"
    },
    {
        "prefix": "from matplotlib import pyplot as plt\nimport numpy\n\n# !FUNCTIONS!\n\ndef generate_start_points(num_points, lower_interval_bound, upper_interval_bound):\n    points_set = set() #use set so we have unique points\n\n    while len(points_set) < num_points: #untill points_set is not full - based on num_points\n        #generate random point (x,y)\n        x = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n        y = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n\n        points_set.add((x, y)) #add x and y as a tuple into the set\n\n    return numpy.array(list(points_set)) # convert set to numpy array and return\ndef generate_additional_points(default_points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty):\n    points = default_points.copy() #copy numpy array and work with its copy\n\n    for _ in range(num_additional_points):\n        # choose random point from existing points\n        random_index = numpy.random.randint(len(points)) \n        random_point = points[random_index]\n        x, y = random_point\n\n        # edit offset interval if too close to edge\n        offset_interval = 100\n        if  x < lower_interval_bound + 100 or x > upper_interval_bound-100 or y < lower_interval_bound + 100 or y > upper_interval_bound-100:\n            offset_interval -= close_to_edge_penalty\n\n        # generate offset\n        X_offset = numpy.random.randint(-offset_interval, offset_interval)\n        Y_offset = numpy.random.randint(-offset_interval, offset_interval)\n\n        # set new point\n        new_x = x+X_offset\n        new_y = y+Y_offset\n\n        points = numpy.append(points, numpy.array([[new_x, new_y]]), axis = 0) # append new point to the points\n    return points\n\ndef visualise_clusters(clusters, medoids):\n    # colors to choose from\n    colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\", \"pink\", \"brown\", \"cyan\", \"magenta\", \"lime\", \"indigo\", \"violet\", \"gold\", \"silver\", \"turquoise\", \"maroon\", \"navy\", \"coral\", \"teal\"]\n    plt.figure(figsize=(10, 10))\n\n    # for each cluster, show one point on matplotlib\n    for i in range(len(clusters)):\n        cluster = clusters[i]\n        if len(cluster) > 0:\n            cluster_points = numpy.array(cluster)\n            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i % len(colors)], s=10, label='Cluster {}'.format(i+1))\n\n    # plot centroids with larger size and different color\n    medoids = numpy.array(medoids)\n    plt.scatter(medoids[:, 0], medoids[:, 1], c='black', s=100, marker='x', label='Centroids', alpha=0.7)\n\n    # aditional matplotlib settings\n    plt.xlim(-5000, 5000)\n    plt.ylim(-5000, 5000)\n    plt.title(\"Divisive CLustering (centroid - K-Means)\")\n    plt.xlabel(\"X Coordinate\")\n    plt.ylabel(\"Y Coordinate\")\n    plt.legend()\n    plt.grid(False)\n    plt.show()\n\ndef euclidean_distance(point, other_point):\n    # calculate euclidian distance based on formula \n    return numpy.sqrt(numpy.sum(((other_point - point) ** 2)))\n\ndef evaluate_cluster_average_mean(clusters, centroids, k, print_logs):\n    ## if print_logs is False -> this function is used for calculation during divisive clustering\n    ## else if print_logs is True -> this function is used in the end of code to evaluare average mean\n\n    succesful_clusters = 0\n\n    # calculate the sum of distances between each point in the cluster and the centroid of that cluster\n    for i, cluster in enumerate(clusters):\n        if len(cluster) > 0:\n            # Sum up the distances from each point in the cluster to its centroid\n            total_distance = sum(euclidean_distance(point, centroids[i]) for point in cluster)\n            avg_distance = total_distance / len(cluster)\n            if avg_distance <= 500:\n                succesful_clusters += 1\n            if print_logs:\n                print(f\"Average distance for cluster {i + 1}: {avg_distance:.2f}\")\n        else: \n            if print_logs:\n                print(f\"Cluster {i+1} was left empty\")\n\n    if print_logs:\n        print(f\"Number of cluster with average distance from centroid smaller than 500 is {succesful_clusters} from {k} possible\")\n\n    return succesful_clusters, k\n\ndef k_means(points, k, lower_interval_bound, upper_interval_bound):\n    # choose k random centroids\n    centroids = points[numpy.random.choice(points.shape[0], k, replace=False)]\n\n    #asign points to the nearest centroid\n    i = 1\n    \n    current_avg = 0\n    previous_avg = 0\n\n    no_update_count = 0\n    average_distances = 0\n    average_distances_count = 0\n    while True:\n        print(f\"Iteration no.: {i}\")\n        i -=- 1\n\n        clusters = [[] for _ in range(k)] #define k clusters\n        distances_to_points = []\n        max_distances = [0] * k\n\n        # put each point to cluster\n        for point in points:\n            all_distances = [euclidean_distance(point, centroid) for centroid in centroids]\n\n            average_distances += sum(all_distances)\n            average_distances_count += len(all_distances)\n\n            closest_centroid = numpy.argmin(all_distances)\n            distances_to_points.append(all_distances[closest_centroid]) \n            clusters[closest_centroid].append(point)\n\n            # Update max distance for this centroid\n            max_distances[closest_centroid] = max(max_distances[closest_centroid], all_distances[closest_centroid])\n\n        #recalculate centroids\n        new_centroids = []\n        for cluster in clusters:\n            if len(cluster) > 0:  # Only calculate the mean for non-empty clusters\n                new_centroids.append(numpy.mean(cluster, axis=0))\n            else:\n                # print(\"bububu\")\n                new_centroids.append([numpy.random.randint(lower_interval_bound, upper_interval_bound), numpy.random.randint(lower_interval_bound, upper_interval_bound)])\n\n        if abs(previous_avg - current_avg) < 1: \n            no_update_count += 1\n        else:\n            no_update_count = 0  # Reset if assignments changed\n\n\n        # Stop if there's no update for 3 iterations\n        if no_update_count >= 3:\n            break\n\n        # Update centroids for the next iteration\n        centroids = new_centroids\n        previous_avg = current_avg\n\n    return clusters, centroids\n####\n\ndef divisive_clustering(points, lower_interval_bound, upper_interval_bound):\n\n    clusters = [points] # all points are one cluster at first\n    centroids = [numpy.mean(points, axis=0)]\n\n    while True:\n        # Evaluate the current clusters\n        successful_clusters, total_clusters = evaluate_cluster_average_mean(clusters, centroids, len(clusters), False)\n        \n        # if each clusters average mean is less than 500, stop the clustering \n        if successful_clusters == total_clusters:\n            print(\"All clusters have average less than 500.\")\n            break\n\n        # find the biggest cluster by average distance to its centroid\n        max_distance_cluster_index = -1\n        max_distance_avg = -1\n        for i, cluster in enumerate(clusters):\n            if len(cluster) > 0:\n                centroid = centroids[i]\n                avg_distance = sum(euclidean_distance(point, centroid) for point in cluster) / len(cluster)\n                if avg_distance > max_distance_avg:\n                    max_distance_avg = avg_distance\n                    max_distance_cluster_index = i\n\n        # split the largest cluster if its average distance is above the threshold\n        if max_distance_avg > 500:\n            largest_cluster = clusters.pop(max_distance_cluster_index)\n            centroids.pop(max_distance_cluster_index)  # Remove the centroid of the largest cluster\n\n            # split the biggest cluster with k_means with centroid\n            sub_clusters, sub_medoids = k_means(numpy.array(largest_cluster), 2, lower_interval_bound, upper_interval_bound)\n\n            # add the split clusters back to the main list of clusters\n            clusters.extend(sub_clusters)\n            centroids.extend(sub_medoids)\n        else:\n            break  # stop if all clusters are below the threshold\n\n\n\n\n    return clusters, centroids\n \n\n# !START HERE!\n\n# important variables\nnum_points = 20\nlower_interval_bound = -5000\nupper_interval_bound = 5000\n\nnum_additional_points = 40000\nclose_to_edge_penalty = 10\n\n# inicialize first points\npoints = generate_start_points(num_p",
        "middle": "oints, lower_interval_boun",
        "suffix": "d, upper_interval_bound) \n# find additional points\nadditional_points = generate_additional_points(points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty) \n\n# start divisive clustering\nclusters, centroids = divisive_clustering(additional_points, lower_interval_bound, upper_interval_bound)\n# print the average mean \nevaluate_cluster_average_mean(clusters, centroids, len(centroids), True)\nprint(\"buliding visualisation...\")\n# visualise clustering\nvisualise_clusters(clusters, centroids)\n"
    },
    {
        "prefix": "import torch.nn as nn  # all neural network modules\nimport torch.nn.functional as functional\n\n\nclass NN (nn.Module): # inherits nn.Module (NN = Neural Network)\n    def __init__(self, input_size, num_c",
        "middle": "lasses): # for mnist ",
        "suffix": "it is 28*28 = 784\n        super(NN, self).__init__()\n\n        # self.hidden_layer_number = 50\n        # self.fc1 = nn.Linear(input_size, self.hidden_layer_number) # 50 - random number for the hidden layer\n        # self.fc2 = nn.Linear(self.hidden_layer_number, num_classes)\n\n        # two hidden layers\n        self.fc1 = nn.Linear(input_size, 250)\n        self.fc2 = nn.Linear(250, 60)\n        self.fc3 = nn.Linear(60, num_classes)\n\n    def forward(self, tensor):\n        # tensor = functional.relu(self.fc1(tensor))\n        # tensor = self.fc2(tensor)\n        # return tensor\n        # return self.fc2(functional.relu(self.fc1(tensor)))\n        return self.fc3(functional.tanh(self.fc2(functional.tanh(self.fc1(tensor)))))\n"
    },
    {
        "prefix": "import torch.nn as nn  # all neural network modules\nimport torch.nn.functional as functional\n\n\nclass NN (nn.Module): # inherits nn.Module (NN = Neural Network)\n    def __init__(self, input_size, num_classes): # for mnist it is 28*28 = 784\n        super(NN, self).__init__()\n\n        # self.hidden_layer_number = 50\n        # self.fc1 = nn.Linear(input_",
        "middle": "size, self.hidden_layer_numb",
        "suffix": "er) # 50 - random number for the hidden layer\n        # self.fc2 = nn.Linear(self.hidden_layer_number, num_classes)\n\n        # two hidden layers\n        self.fc1 = nn.Linear(input_size, 250)\n        self.fc2 = nn.Linear(250, 60)\n        self.fc3 = nn.Linear(60, num_classes)\n\n    def forward(self, tensor):\n        # tensor = functional.relu(self.fc1(tensor))\n        # tensor = self.fc2(tensor)\n        # return tensor\n        # return self.fc2(functional.relu(self.fc1(tensor)))\n        return self.fc3(functional.tanh(self.fc2(functional.tanh(self.fc1(tensor)))))\n"
    },
    {
        "prefix": "import torch.nn as nn  # all neural network modules\nim",
        "middle": "port torch.nn.functiona",
        "suffix": "l as functional\n\n\nclass NN (nn.Module): # inherits nn.Module (NN = Neural Network)\n    def __init__(self, input_size, num_classes): # for mnist it is 28*28 = 784\n        super(NN, self).__init__()\n\n        # self.hidden_layer_number = 50\n        # self.fc1 = nn.Linear(input_size, self.hidden_layer_number) # 50 - random number for the hidden layer\n        # self.fc2 = nn.Linear(self.hidden_layer_number, num_classes)\n\n        # two hidden layers\n        self.fc1 = nn.Linear(input_size, 250)\n        self.fc2 = nn.Linear(250, 60)\n        self.fc3 = nn.Linear(60, num_classes)\n\n    def forward(self, tensor):\n        # tensor = functional.relu(self.fc1(tensor))\n        # tensor = self.fc2(tensor)\n        # return tensor\n        # return self.fc2(functional.relu(self.fc1(tensor)))\n        return self.fc3(functional.tanh(self.fc2(functional.tanh(self.fc1(tensor)))))\n"
    },
    {
        "prefix": "import torch.nn as nn  # all neural network modules\nimport torch.nn.functional as functional\n\n\nclass NN (nn.Module): # inherits nn.Module (NN = Neural Network)\n    de",
        "middle": "f __init__(self",
        "suffix": ", input_size, num_classes): # for mnist it is 28*28 = 784\n        super(NN, self).__init__()\n\n        # self.hidden_layer_number = 50\n        # self.fc1 = nn.Linear(input_size, self.hidden_layer_number) # 50 - random number for the hidden layer\n        # self.fc2 = nn.Linear(self.hidden_layer_number, num_classes)\n\n        # two hidden layers\n        self.fc1 = nn.Linear(input_size, 250)\n        self.fc2 = nn.Linear(250, 60)\n        self.fc3 = nn.Linear(60, num_classes)\n\n    def forward(self, tensor):\n        # tensor = functional.relu(self.fc1(tensor))\n        # tensor = self.fc2(tensor)\n        # return tensor\n        # return self.fc2(functional.relu(self.fc1(tensor)))\n        return self.fc3(functional.tanh(self.fc2(functional.tanh(self.fc1(tensor)))))\n"
    },
    {
        "prefix": "import torch.nn as nn  # all neural network modules\nimport torch.nn.functional as functional\n\n\nclass NN (nn.Module): # inherits nn.Module (NN = Neural Network)\n    def __init__(self, input_size, num_classes): # for mnist it is 28*28 = 784\n        super(NN, self).__init__()\n\n        # self.hidden_layer_number = 50\n        # self.fc1 ",
        "middle": "= nn.Linear(input",
        "suffix": "_size, self.hidden_layer_number) # 50 - random number for the hidden layer\n        # self.fc2 = nn.Linear(self.hidden_layer_number, num_classes)\n\n        # two hidden layers\n        self.fc1 = nn.Linear(input_size, 250)\n        self.fc2 = nn.Linear(250, 60)\n        self.fc3 = nn.Linear(60, num_classes)\n\n    def forward(self, tensor):\n        # tensor = functional.relu(self.fc1(tensor))\n        # tensor = self.fc2(tensor)\n        # return tensor\n        # return self.fc2(functional.relu(self.fc1(tensor)))\n        return self.fc3(functional.tanh(self.fc2(functional.tanh(self.fc1(tensor)))))\n"
    },
    {
        "prefix": "from matplotlib import pyplot as plt\nimport numpy\n\n# !FUNCTIONS!\n\ndef generate_start_points(num_points, lower_interval_bound, upper_int",
        "middle": "erval_bound):\n ",
        "suffix": "   points_set = set() # use set so we have unique points\n\n    while len(points_set) < num_points: # untill points_set is not full - based on num_points\n        # generate random point (x,y)\n        x = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n        y = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n\n        points_set.add((x, y)) #add x and y as a tuple into the set\n\n    return numpy.array(list(points_set)) # convert set to numpy array and return\ndef generate_additional_points(default_points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty):\n    points = default_points.copy() #copy numpy array and work with its copy\n\n    for _ in range(num_additional_points):\n        # choose random point from existing points\n        random_index = numpy.random.randint(len(points))\n        random_point = points[random_index]\n        x, y = random_point\n\n        # edit offset interval if too close to edge\n        offset_interval = 100\n        if  x < lower_interval_bound + 100 or x > upper_interval_bound-100 or y < lower_interval_bound + 100 or y > upper_interval_bound-100:\n            offset_interval -= close_to_edge_penalty\n\n         # generate offset\n        X_offset = numpy.random.randint(-offset_interval, offset_interval)\n        Y_offset = numpy.random.randint(-offset_interval, offset_interval)\n\n\n        # set new point\n        new_x = x+X_offset\n        new_y = y+Y_offset\n\n        # append new point to the points\n        points = numpy.append(points, numpy.array([[new_x, new_y]]), axis = 0)\n\n    return points\ndef visualise_clusters(clusters, centroids):\n    # colors to choose from\n    colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\", \"pink\", \"brown\", \"cyan\", \"magenta\", \"lime\", \"indigo\", \"violet\", \"gold\", \"silver\", \"turquoise\", \"maroon\", \"navy\", \"coral\", \"teal\"]\n    plt.figure(figsize=(10, 10))\n\n     # for each cluster, show its points on matplotlib\n    for i in range(len(clusters)):\n        cluster = clusters[i]\n        if len(cluster) > 0:\n            cluster_points = numpy.array(cluster)\n            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i % len(colors)], s=10, label='Cluster {}'.format(i+1))\n\n    # Plot centroids with larger size and different color\n    centroids = numpy.array(centroids)\n    plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=100, marker='x', label='Centroids')\n\n    # aditional matplotlib settings\n    plt.xlim(-5000, 5000)\n    plt.ylim(-5000, 5000)\n    plt.title(\"K-Means Clustering (centroid)\")\n    plt.xlabel(\"X Coordinate\")\n    plt.ylabel(\"Y Coordinate\")\n    plt.legend()\n    plt.grid(False)\n    plt.show()\n\ndef euclidean_distance(point, centroid):\n    # calculate euclidian distance based on formula \n    return numpy.sqrt(numpy.sum(((centroid - point) ** 2)))\n\ndef k_means(points, k, lower_interval_bound, upper_interval_bound):\n    # choose k random centroids \n    centroids = []\n    for _ in range(k):\n        centroids.append([numpy.random.randint(lower_interval_bound, upper_interval_bound), numpy.random.randint(lower_interval_bound, upper_interval_bound)])\n\n    #asign points to the nearest centroid\n    i = 1\n    \n    current_avg = 0\n    previous_avg = 0\n\n    no_update_count = 0\n    average_distances = 0\n    average_distances_count = 0\n    while True:\n        print(f\"Iteration no.: {i}\")\n        i -=- 1\n\n        clusters = [[] for _ in range(k)] #define k clusters\n        distances_to_points = []\n        max_distances = [0] * k\n\n        # put each point to cluster\n        for point in points:\n            all_distances = [euclidean_distance(point, centroid) for centroid in centroids]\n\n            average_distances += sum(all_distances)\n            average_distances_count += len(all_distances)\n\n            closest_centroid = numpy.argmin(all_distances)\n            distances_to_points.append(all_distances[closest_centroid]) \n            clusters[closest_centroid].append(point)\n\n            # Update max distance for this centroid\n            max_distances[closest_centroid] = max(max_distances[closest_centroid], all_distances[closest_centroid])\n\n\n        # Calculate the average distance for this iteration\n        if distances_to_points:\n            average_distances = sum(distances_to_points)\n            average_distances_count = len(distances_to_points)\n            current_avg = average_distances / average_distances_count\n            print(f\"Average distance to closest centroid: {current_avg:.2f}\")\n\n        #recalculate centroids\n        new_centroids = []\n        for cluster in clusters:\n            if len(cluster) > 0:  # Only calculate the mean for non-empty clusters\n                new_centroids.append(numpy.mean(cluster, axis=0))\n            else:\n                new_centroids.append([numpy.random.randint(lower_interval_bound, upper_interval_bound), numpy.random.randint(lower_interval_bound, upper_interval_bound)])\n\n        if no_update_count > 0:\n            print(f\"No update for {no_update_count}/3\")\n\n        if abs(previous_avg - current_avg) < 1:\n            no_update_count += 1\n        else:\n            no_update_count = 0  # Reset if assignments have changed\n\n\n        # Stop if the maximum distance is less than 500 or if there's no update for 3 iterations\n        if no_update_count >= 3:\n            print(f\"    very small update for {no_update_count} iterations, not worth to continue\")\n            break\n\n        # Update centroids for the next iteration\n        centroids = new_centroids\n        previous_avg = current_avg\n\n    return clusters, centroids\n\ndef evaluate_cluster_average_mean(clusters, centroids, k):\n\n    succesful_clusters = 0\n\n    # calculate the sum of distances between each point in the cluster and the centroid of that cluster\n    for i, cluster in enumerate(clusters):\n        if len(cluster) > 0:\n            # Sum up the distances from each point in the cluster to its centroid\n            total_distance = sum(euclidean_distance(point, centroids[i]) for point in cluster)\n            avg_distance = total_distance / len(cluster)\n            if avg_distance <= 500:\n                succesful_clusters += 1\n            print(f\"Average distance for cluster {i + 1}: {avg_distance:.2f}\")\n        else: \n            print(f\"Cluster {i+1} was left empty\")\n\n    print(f\"Number of cluster with average distance from centroid smaller than 500 is {succesful_clusters} from {k} possible\")\n\n    return\n####\n\n \n# !START HERE!\n\n#important variables\nnum_points = 20\nlower_interval_bound = -5000\nupper_interval_bound = 5000\n\nnum_additional_points = 40000\nclose_to_edge_penalty = 10\n\nk = 15 #number of clusters\n\n#inicialize first points\npoints = generate_start_points(num_points, lower_interval_bound, upper_interval_bound) \n# find additional points\nadditional_points = generate_additional_points(points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty) \n\nclusters, centroids = k_means(additional_points, k, lower_interval_bound, upper_interval_bound)\nevaluate_cluster_average_mean(clusters, centroids, k)\nprint(\"buliding visualisation...\")\nvisualise_clusters(clusters, centroids)\n"
    },
    {
        "prefix": "from matplotlib import pyplot as plt\nimport numpy\n\n# !FUNCTIONS!\n\ndef generate_start_points(num_points, lower_interval_bound, upper_interval_bound):\n    points_set = set() # use set so we have unique points\n\n    while len(points_set) < num_points: # untill points_set is not full - based on num_points\n        # generate random point (x,y)\n        x = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n        y = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n\n        points_set.add((x, y)) #add x and y as a tuple into the set\n\n    return numpy.array(list(points_set)) # convert set to numpy array and return\ndef generate_additional_points(default_points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty):\n    points = default_points.copy() #copy numpy array and work with its copy\n\n    for _ in range(num_additional_points):\n        # choose random point from existing points\n        random_index = numpy.random.randint(len(points))\n        random_point = points[random_index]\n        x, y = random_point\n\n        # edit offset interval if too close to edge\n        offset_interval = 100\n        if  x < lower_interval_bound + 100 or x > upper_interval_bound-100 or y < lower_interval_bound + 100 or y > upper_interval_bound-100:\n            offset_interval -= close_to_edge_penalty\n\n         # generate offset\n        X_offset = numpy.random.randint(-offset_interval, offset_interval)\n        Y_offset = numpy.random.randint(-offset_interval, offset_interval)\n\n\n        # set new point\n        new_x = x+X_offset\n        new_y = y+Y_offset\n\n        # append new point to the points\n        points = numpy.append(points, numpy.array([[new_x, new_y]]), axis = 0)\n\n    return points\ndef visualise_clusters(clusters, centroids):\n    # colors to choose from\n    colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\", \"pink\", \"brown\", \"cyan\", \"magenta\", \"lime\", \"indigo\", \"violet\", \"gold\", \"silver\", \"turquoise\", \"maroon\", \"navy\", \"coral\", \"teal\"]\n    plt.figure(figsize=(10, 10))\n\n     # for each cluster, show its points on matplotlib\n    for i in range(len(clusters)):\n        cluster = clusters[i]\n        if len(cluster) > 0:\n            cluster_points = numpy.array(cluster)\n            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i % len(colors)], s=10, label='Cluster {}'.format(i+1))\n\n    # Plot centroids with larger size and different color\n    centroids = numpy.array(centroids)\n    plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=100, marker='x', label='Centroids')\n\n    # aditional matplotlib settings\n    plt.xlim(-5000, 5000)\n    plt.ylim(-5000, 5000)\n    plt.title(\"K-Means Clustering (centroid)\")\n    plt.xlabel(\"X Coordinate\")\n    plt.ylabel(\"Y Coordinate\")\n    plt.legend()\n    plt.grid(False)\n    plt.show()\n\ndef euclidean_distance(point, centroid):\n    # calculate euclidian distance based on formula \n    return numpy.sqrt(numpy.sum(((centroid - point) ** 2)))\n\ndef k_means(points, k, lower_interval_bound, upper_interval_bound):\n    # choose k random centroids \n    centroids = []\n    for _ in range(k):\n        centroids.append([numpy.random.randint(lower_interval_bound, upper_interval_bound), numpy.random.randint(lower_interval_bound, upper_interval_bound)])\n\n    #asign points to the nearest centroid\n    i = 1\n    \n    current_avg = 0\n    previous_avg = 0\n\n    no_update_count = 0\n    average_distances = 0\n    average_distances_count = 0\n    while True:\n        print(f\"Iteration no.: {i}\")\n        i -=- 1\n\n        clusters = [[] for _ in range(k)] #define k clusters\n        distances_to_points = []\n        max_distances = [0] * k\n\n        # put each point to cluster\n        for point in points:\n            all_distances = [euclidean_distance(point, centroid) for centroid in centroids]\n\n            average_distances += sum(all_distances)\n            average_distances_count += len(all_distances)\n\n            closest_centroid = numpy.argmin(all_distances)\n            distances_to_points.append(all_distances[closest_centroid]) \n            clusters[closest_centroid].append(point)\n\n            # Update max distance for this centroid\n            max_distances[closest_centroid] = max(max_distances[closest_centroid], all_distances[closest_centroid])\n\n\n        # Calculate the average distance for this iteration\n        if distances_to_points:\n            average_distances = sum(distances_to_points)\n            average_distances_count = len(distances_to_points)\n            current_avg = average_distances / average_distances_count\n            print(f\"Average distance to closest centroid: {current_avg:.2f}\")\n\n        #recalculate centroids\n        new_centroids = []\n        for cluster in clusters:\n            if len(cluster) > 0:  # Only calculate the mean for non-empty clusters\n                new_centroids.append(numpy.mean(cluster, axis=0))\n            else:\n                new_centroids.append([numpy.random.randint(lower_interval_bound, upper_interval_bound), numpy.random.randint(lower_interval_bound, upper_interval_bound)])\n\n        if no_update_count > 0:\n            print(f\"No update for {no_update_count}/3\")\n\n        if abs(previous_avg - current_avg) < 1:\n            no_update_count += 1\n        else:\n            no_update_count = 0  # Reset if assignments have changed\n\n\n        # Stop if the maximum distance is less than 500 or if there's no update for 3 iterations\n        if no_update_count >= 3:\n            print(f\"    very small update for {no_update_count} iterations, not worth to continue\")\n            break\n\n        # Update centroids for the next iteration\n        centroids = new_centroids\n        previous_avg = current_avg\n\n    return clusters, centroids\n\ndef evaluate_cluster_average_mean(clusters, centroids, k):\n\n    succesful_clusters = 0\n\n    # calculate the sum of distances between each point in the cluster and the centroid of that cluster\n    for i, cluster in enumerate(clusters):\n        if len(cluster) > 0:\n            # Sum up the distances from each point in the cluster to its centroid\n            total_distance = sum(euclidean_distance(",
        "middle": "point, centroids[i",
        "suffix": "]) for point in cluster)\n            avg_distance = total_distance / len(cluster)\n            if avg_distance <= 500:\n                succesful_clusters += 1\n            print(f\"Average distance for cluster {i + 1}: {avg_distance:.2f}\")\n        else: \n            print(f\"Cluster {i+1} was left empty\")\n\n    print(f\"Number of cluster with average distance from centroid smaller than 500 is {succesful_clusters} from {k} possible\")\n\n    return\n####\n\n \n# !START HERE!\n\n#important variables\nnum_points = 20\nlower_interval_bound = -5000\nupper_interval_bound = 5000\n\nnum_additional_points = 40000\nclose_to_edge_penalty = 10\n\nk = 15 #number of clusters\n\n#inicialize first points\npoints = generate_start_points(num_points, lower_interval_bound, upper_interval_bound) \n# find additional points\nadditional_points = generate_additional_points(points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty) \n\nclusters, centroids = k_means(additional_points, k, lower_interval_bound, upper_interval_bound)\nevaluate_cluster_average_mean(clusters, centroids, k)\nprint(\"buliding visualisation...\")\nvisualise_clusters(clusters, centroids)\n"
    },
    {
        "prefix": "from matplotlib import pyplot as plt\nimport numpy\n\n# !FUNCTIONS!\n\ndef generate_start_points(num_points, lower_interval_bound, upper_interval_bound):\n    points_set = set() # use set so we have unique points\n\n    while len(points_set) < num_points: # untill points_set is not full - based on num_points\n        # generate random point (x,y)\n        x = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n        y = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n\n        points_set.add((x, y)) #add x and y as a tuple into the set\n\n    return numpy.array(list(points_set)) # convert set to numpy array and return\ndef generate_additional_points(default_points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty):\n    points = default_points.copy() #copy numpy array and work with its copy\n\n    for _ in range(num_additional_points):\n        # choose random point from existing points\n        random_index = numpy.random.randint(len(points))\n        random_point = points[random_index]\n        x, y = random_point\n\n        # edit offset interval if too close to edge\n        offset_interval = 100\n        if  x < lower_interval_bound + 100 or x > upper_interval_bound-100 or y < lower_interval_bound + 100 or y > upper_interval_bound-100:\n            offset_interval -= close_to_edge_penalty\n\n         # generate offset\n        X_offset = numpy.random.randint(-offset_interval, offset_interval)\n        Y_offset = numpy.random.randint(-offset_interval, offset_interval)\n\n\n        # set new point\n        new_x = x+X_offset\n        new_y = y+Y_offset\n\n        # append new point to the points\n        points = numpy.append(points, numpy.array([[new_x, new_y]]), axis = 0)\n\n    return points\ndef visualise_clusters(clusters, centroids):\n    # colors to choose from\n    colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\", \"pink\", \"brown\", \"cyan\", \"magenta\", \"lime\", \"indigo\", \"violet\", \"gold\", \"silver\", \"turquoise\", \"maroon\", \"navy\", \"coral\", \"teal\"]\n    plt.figure(figsize=(10, 10))\n\n     # for each cluster, show its points on matplotlib\n    for i in range(len(clusters)):\n        cluster = clusters[i]\n        if len(cluster) > 0:\n            cluster_points = numpy.array(cluster)\n            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i % len(colors)], s=10, label='Cluster {}'.format(i+1))\n\n    # Plot centroids with larger size and different color\n    centroids = numpy.array(centroids)\n    plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=100, marker='x', label='Centroids')\n\n    # aditional matplotlib settings\n    plt.xlim(-5000, 5000)\n    plt.ylim(-5000, 5000)\n    plt.title(\"K-Means Clustering (centroid)\")\n    plt.xlabel(\"X Coordinate\")\n    plt.ylabel(\"Y Coordinate\")\n    plt.legend()\n    plt.grid(False)\n    plt.show()\n\ndef euclidean_distance(point, centroid):\n    # calculate euclidian distance based on formula \n    return numpy.sqrt(numpy.sum(((centroid - point) ** 2)))\n\ndef k_means(points, k, lower_interval_bound, upper_interval_bound):\n    # choose k random centroids \n    centroids = []\n    for _ in range(k):\n     ",
        "middle": "   centroids.append([numpy.",
        "suffix": "random.randint(lower_interval_bound, upper_interval_bound), numpy.random.randint(lower_interval_bound, upper_interval_bound)])\n\n    #asign points to the nearest centroid\n    i = 1\n    \n    current_avg = 0\n    previous_avg = 0\n\n    no_update_count = 0\n    average_distances = 0\n    average_distances_count = 0\n    while True:\n        print(f\"Iteration no.: {i}\")\n        i -=- 1\n\n        clusters = [[] for _ in range(k)] #define k clusters\n        distances_to_points = []\n        max_distances = [0] * k\n\n        # put each point to cluster\n        for point in points:\n            all_distances = [euclidean_distance(point, centroid) for centroid in centroids]\n\n            average_distances += sum(all_distances)\n            average_distances_count += len(all_distances)\n\n            closest_centroid = numpy.argmin(all_distances)\n            distances_to_points.append(all_distances[closest_centroid]) \n            clusters[closest_centroid].append(point)\n\n            # Update max distance for this centroid\n            max_distances[closest_centroid] = max(max_distances[closest_centroid], all_distances[closest_centroid])\n\n\n        # Calculate the average distance for this iteration\n        if distances_to_points:\n            average_distances = sum(distances_to_points)\n            average_distances_count = len(distances_to_points)\n            current_avg = average_distances / average_distances_count\n            print(f\"Average distance to closest centroid: {current_avg:.2f}\")\n\n        #recalculate centroids\n        new_centroids = []\n        for cluster in clusters:\n            if len(cluster) > 0:  # Only calculate the mean for non-empty clusters\n                new_centroids.append(numpy.mean(cluster, axis=0))\n            else:\n                new_centroids.append([numpy.random.randint(lower_interval_bound, upper_interval_bound), numpy.random.randint(lower_interval_bound, upper_interval_bound)])\n\n        if no_update_count > 0:\n            print(f\"No update for {no_update_count}/3\")\n\n        if abs(previous_avg - current_avg) < 1:\n            no_update_count += 1\n        else:\n            no_update_count = 0  # Reset if assignments have changed\n\n\n        # Stop if the maximum distance is less than 500 or if there's no update for 3 iterations\n        if no_update_count >= 3:\n            print(f\"    very small update for {no_update_count} iterations, not worth to continue\")\n            break\n\n        # Update centroids for the next iteration\n        centroids = new_centroids\n        previous_avg = current_avg\n\n    return clusters, centroids\n\ndef evaluate_cluster_average_mean(clusters, centroids, k):\n\n    succesful_clusters = 0\n\n    # calculate the sum of distances between each point in the cluster and the centroid of that cluster\n    for i, cluster in enumerate(clusters):\n        if len(cluster) > 0:\n            # Sum up the distances from each point in the cluster to its centroid\n            total_distance = sum(euclidean_distance(point, centroids[i]) for point in cluster)\n            avg_distance = total_distance / len(cluster)\n            if avg_distance <= 500:\n                succesful_clusters += 1\n            print(f\"Average distance for cluster {i + 1}: {avg_distance:.2f}\")\n        else: \n            print(f\"Cluster {i+1} was left empty\")\n\n    print(f\"Number of cluster with average distance from centroid smaller than 500 is {succesful_clusters} from {k} possible\")\n\n    return\n####\n\n \n# !START HERE!\n\n#important variables\nnum_points = 20\nlower_interval_bound = -5000\nupper_interval_bound = 5000\n\nnum_additional_points = 40000\nclose_to_edge_penalty = 10\n\nk = 15 #number of clusters\n\n#inicialize first points\npoints = generate_start_points(num_points, lower_interval_bound, upper_interval_bound) \n# find additional points\nadditional_points = generate_additional_points(points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty) \n\nclusters, centroids = k_means(additional_points, k, lower_interval_bound, upper_interval_bound)\nevaluate_cluster_average_mean(clusters, centroids, k)\nprint(\"buliding visualisation...\")\nvisualise_clusters(clusters, centroids)\n"
    },
    {
        "prefix": "from matplotlib import pyplot as plt\nimport numpy\n\n# !FUNCTIONS!\n\ndef generate_start_points(num_points, lower_interval_bound, upper_interval_bound):\n    points_set = set() # use set so we have unique points\n\n    while len(points_set) < num_points: # untill points_set is not full - based on num_points\n        # generate random point (x,y)\n        x = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n        y = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n\n        points_set.add((x, y)) #add x and y as a tuple into the set\n\n    return numpy.array(list(points_set)) # convert set to numpy array and return\ndef generate_additional_points(default_points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty):\n    points = default_points.copy() #copy numpy array and work with its copy\n\n    for _ in range(num_additional_points):\n        # choose random point from existing points\n        random_index = numpy.random.randint(len(points))\n        random_point = points[random_index]\n        x, y = random_point\n\n        # edit offset interval if too close to edge\n        offset_interval = 100\n        if  x < lower_interval_bound + 100 or x > upper_interval_bound-100 or y < lower_interval_bound + 100 or y > upper_interval_bound-100:\n            offset_interval -= close_to_edge_penalty\n\n         # generate offset\n        X_offset = numpy.random.randint(-offset_interval, offset_interval)\n        Y_offset = numpy.random.randint(-offset_interval, offset_interval)\n\n\n        # set new point\n        new_x = x+X_offset\n        new_y = y+Y_offset\n\n        # append new point to the points\n        points = numpy.append(points, numpy.array([[new_x, new_y]]), axis = 0)\n\n    return points\ndef visualise_clusters(clusters, centroids):\n    # colors to choose from\n    colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\", \"pink\", \"brown\", \"cyan\", \"magenta\", \"lime\", \"indigo\", \"violet\", \"gold\", \"silver\", \"turquoise\", \"maroon\", \"navy\", \"coral\", \"teal\"]\n    plt.figure(figsize=(10, 10))\n\n     # for each cluster, show its points on matplotlib\n    for i in range(len(clusters)):\n        cluster = clusters[i]\n        if len(cluster) > 0:\n            cluster_points = numpy.array(cluster)\n            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i % len(colors)], s=10, label='Cluster {}'.format(i+1))\n\n    # Plot centroids with larger size and different color\n    centroids = numpy.array(centroids)\n    plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=100, marker='x', label='Centroids')\n\n    # aditional matplotlib settings\n    plt.xlim(-5000, 5000)\n    plt.ylim(-5000, 5000)\n    plt.title(\"K-Means Clustering (centroid)\")\n    plt.xlabel(\"X Coordinate\")\n    plt.ylabel(\"Y Coordinate\")\n    plt.legend()\n    plt.grid(False)\n    plt.show()\n\ndef euclidean_distance(point, centroid):\n    # calculate euclidian distance based on formula \n    return numpy.sqrt(numpy.sum(((centroid - point) ** 2)))\n\ndef k_means(points, k, lower_interval_bound, upper_interval_bound):\n    # choose k random centroids \n    centroids = []\n    for _ in range(k):\n        centroids.append([numpy.random.randint(lower_interval_bound, upper_interval_bound), numpy.random.randint(lower_interval_bound, upper_interval_bound)])\n\n    #asign points to the nearest centroid\n    i = 1\n    \n    current_avg = 0\n    previous_avg = 0\n\n    no_update_count = 0\n    average_distances = 0\n    average_distances_count = 0\n    while True:\n        print(f\"Iteration no.: {i}\")\n        i -=- 1\n\n        clusters = [[] for _ in range(k)] #define k clusters\n        distances_to_points = []\n        max_distances = [0] * k\n\n        # put each point to cluster\n        for point in points:\n            all_distances = [euclidean_distance(point, centroid) for centroid in centroids]\n\n            average_distances += sum(all_distances)\n            average_distances_count += len(all_distances)\n\n            closest_centroid = numpy.argmin(all_distances)\n            distances_to_points.append(all_distances[closest_centroid]) \n            clusters[closest_centroid].append(po",
        "middle": "int)\n\n            # Update ",
        "suffix": "max distance for this centroid\n            max_distances[closest_centroid] = max(max_distances[closest_centroid], all_distances[closest_centroid])\n\n\n        # Calculate the average distance for this iteration\n        if distances_to_points:\n            average_distances = sum(distances_to_points)\n            average_distances_count = len(distances_to_points)\n            current_avg = average_distances / average_distances_count\n            print(f\"Average distance to closest centroid: {current_avg:.2f}\")\n\n        #recalculate centroids\n        new_centroids = []\n        for cluster in clusters:\n            if len(cluster) > 0:  # Only calculate the mean for non-empty clusters\n                new_centroids.append(numpy.mean(cluster, axis=0))\n            else:\n                new_centroids.append([numpy.random.randint(lower_interval_bound, upper_interval_bound), numpy.random.randint(lower_interval_bound, upper_interval_bound)])\n\n        if no_update_count > 0:\n            print(f\"No update for {no_update_count}/3\")\n\n        if abs(previous_avg - current_avg) < 1:\n            no_update_count += 1\n        else:\n            no_update_count = 0  # Reset if assignments have changed\n\n\n        # Stop if the maximum distance is less than 500 or if there's no update for 3 iterations\n        if no_update_count >= 3:\n            print(f\"    very small update for {no_update_count} iterations, not worth to continue\")\n            break\n\n        # Update centroids for the next iteration\n        centroids = new_centroids\n        previous_avg = current_avg\n\n    return clusters, centroids\n\ndef evaluate_cluster_average_mean(clusters, centroids, k):\n\n    succesful_clusters = 0\n\n    # calculate the sum of distances between each point in the cluster and the centroid of that cluster\n    for i, cluster in enumerate(clusters):\n        if len(cluster) > 0:\n            # Sum up the distances from each point in the cluster to its centroid\n            total_distance = sum(euclidean_distance(point, centroids[i]) for point in cluster)\n            avg_distance = total_distance / len(cluster)\n            if avg_distance <= 500:\n                succesful_clusters += 1\n            print(f\"Average distance for cluster {i + 1}: {avg_distance:.2f}\")\n        else: \n            print(f\"Cluster {i+1} was left empty\")\n\n    print(f\"Number of cluster with average distance from centroid smaller than 500 is {succesful_clusters} from {k} possible\")\n\n    return\n####\n\n \n# !START HERE!\n\n#important variables\nnum_points = 20\nlower_interval_bound = -5000\nupper_interval_bound = 5000\n\nnum_additional_points = 40000\nclose_to_edge_penalty = 10\n\nk = 15 #number of clusters\n\n#inicialize first points\npoints = generate_start_points(num_points, lower_interval_bound, upper_interval_bound) \n# find additional points\nadditional_points = generate_additional_points(points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty) \n\nclusters, centroids = k_means(additional_points, k, lower_interval_bound, upper_interval_bound)\nevaluate_cluster_average_mean(clusters, centroids, k)\nprint(\"buliding visualisation...\")\nvisualise_clusters(clusters, centroids)\n"
    },
    {
        "prefix": "from matplotlib import pyplot as plt\nimport numpy\n\n# !FUNCTIONS!\n\ndef generate_start_points(num_points, lower_interval_bound, upper_interval_bound):\n    points_set = set() #use set so we have unique points\n\n    while len(points_set) < num_points: #untill points_set is not full - based on num_points\n        #generate random point (x,y)\n        x = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n        y = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n\n        points_set.add((x, y)) #add x and y as a tuple into the set\n\n    return numpy.array(list(points_set)) # convert set to numpy array and return\ndef generate_additional_points(default_points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty):\n    points = default_points.copy() #copy numpy array and work with its copy\n\n    for _ in range(num_additional_points):\n        # choose random point from existing points\n        random_index = numpy.random.randint(len(points)) \n        random_point = points[random_index]\n        x, y = random_point\n\n        # edit offset interval if too close to edge\n        offset_interval = 100\n        if  x < lower_interval_bound + 100 or x > upper_interval_bound-100 or y < lower_interval_bound + 100 or y > upper_interval_bound-100:\n            offset_interval -= close_to_edge_penalty\n\n        # generate offset\n        X_offset = numpy.random.randint(-offset_interval, offset_interval)\n        Y_offset = numpy.random.randint(-offset_interval, offset_interval)\n\n        # set new point\n        new_x = x+X_offset\n        new_y = y+Y_offset\n\n        points = numpy.append(points, numpy.array([[new_x, new_y]]), axis = 0) # append new point to the points\n    return points\n\ndef visualise_clusters(clusters, medoids):\n    # colors to choose from\n    colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\", \"pink\", \"brown\", \"cyan\", \"magenta\", \"lime\", \"indigo\", \"violet\", \"gold\", \"silver\", \"turquoise\", \"maroon\", \"navy\", \"coral\", \"teal\"]\n    plt.figure(figsize=(10, 10))\n\n    # for each cluster, show one point on matplotlib\n    fo",
        "middle": "r i in range(len(clusters)):\n        cluster = clusters[i]\n",
        "suffix": "        if len(cluster) > 0:\n            cluster_points = numpy.array(cluster)\n            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i % len(colors)], s=10, label='Cluster {}'.format(i+1))\n\n    # plot centroids with larger size and different color\n    medoids = numpy.array(medoids)\n    plt.scatter(medoids[:, 0], medoids[:, 1], c='black', s=100, marker='x', label='Centroids', alpha=0.7)\n\n    # aditional matplotlib settings\n    plt.xlim(-5000, 5000)\n    plt.ylim(-5000, 5000)\n    plt.title(\"Divisive CLustering (centroid - K-Means)\")\n    plt.xlabel(\"X Coordinate\")\n    plt.ylabel(\"Y Coordinate\")\n    plt.legend()\n    plt.grid(False)\n    plt.show()\n\ndef euclidean_distance(point, other_point):\n    # calculate euclidian distance based on formula \n    return numpy.sqrt(numpy.sum(((other_point - point) ** 2)))\n\ndef evaluate_cluster_average_mean(clusters, centroids, k, print_logs):\n    ## if print_logs is False -> this function is used for calculation during divisive clustering\n    ## else if print_logs is True -> this function is used in the end of code to evaluare average mean\n\n    succesful_clusters = 0\n\n    # calculate the sum of distances between each point in the cluster and the centroid of that cluster\n    for i, cluster in enumerate(clusters):\n        if len(cluster) > 0:\n            # Sum up the distances from each point in the cluster to its centroid\n            total_distance = sum(euclidean_distance(point, centroids[i]) for point in cluster)\n            avg_distance = total_distance / len(cluster)\n            if avg_distance <= 500:\n                succesful_clusters += 1\n            if print_logs:\n                print(f\"Average distance for cluster {i + 1}: {avg_distance:.2f}\")\n        else: \n            if print_logs:\n                print(f\"Cluster {i+1} was left empty\")\n\n    if print_logs:\n        print(f\"Number of cluster with average distance from centroid smaller than 500 is {succesful_clusters} from {k} possible\")\n\n    return succesful_clusters, k\n\ndef k_means(points, k, lower_interval_bound, upper_interval_bound):\n    # choose k random centroids\n    centroids = points[numpy.random.choice(points.shape[0], k, replace=False)]\n\n    #asign points to the nearest centroid\n    i = 1\n    \n    current_avg = 0\n    previous_avg = 0\n\n    no_update_count = 0\n    average_distances = 0\n    average_distances_count = 0\n    while True:\n        print(f\"Iteration no.: {i}\")\n        i -=- 1\n\n        clusters = [[] for _ in range(k)] #define k clusters\n        distances_to_points = []\n        max_distances = [0] * k\n\n        # put each point to cluster\n        for point in points:\n            all_distances = [euclidean_distance(point, centroid) for centroid in centroids]\n\n            average_distances += sum(all_distances)\n            average_distances_count += len(all_distances)\n\n            closest_centroid = numpy.argmin(all_distances)\n            distances_to_points.append(all_distances[closest_centroid]) \n            clusters[closest_centroid].append(point)\n\n            # Update max distance for this centroid\n            max_distances[closest_centroid] = max(max_distances[closest_centroid], all_distances[closest_centroid])\n\n        #recalculate centroids\n        new_centroids = []\n        for cluster in clusters:\n            if len(cluster) > 0:  # Only calculate the mean for non-empty clusters\n                new_centroids.append(numpy.mean(cluster, axis=0))\n            else:\n                # print(\"bububu\")\n                new_centroids.append([numpy.random.randint(lower_interval_bound, upper_interval_bound), numpy.random.randint(lower_interval_bound, upper_interval_bound)])\n\n        if abs(previous_avg - current_avg) < 1: \n            no_update_count += 1\n        else:\n            no_update_count = 0  # Reset if assignments changed\n\n\n        # Stop if there's no update for 3 iterations\n        if no_update_count >= 3:\n            break\n\n        # Update centroids for the next iteration\n        centroids = new_centroids\n        previous_avg = current_avg\n\n    return clusters, centroids\n####\n\ndef divisive_clustering(points, lower_interval_bound, upper_interval_bound):\n\n    clusters = [points] # all points are one cluster at first\n    centroids = [numpy.mean(points, axis=0)]\n\n    while True:\n        # Evaluate the current clusters\n        successful_clusters, total_clusters = evaluate_cluster_average_mean(clusters, centroids, len(clusters), False)\n        \n        # if each clusters average mean is less than 500, stop the clustering \n        if successful_clusters == total_clusters:\n            print(\"All clusters have average less than 500.\")\n            break\n\n        # find the biggest cluster by average distance to its centroid\n        max_distance_cluster_index = -1\n        max_distance_avg = -1\n        for i, cluster in enumerate(clusters):\n            if len(cluster) > 0:\n                centroid = centroids[i]\n                avg_distance = sum(euclidean_distance(point, centroid) for point in cluster) / len(cluster)\n                if avg_distance > max_distance_avg:\n                    max_distance_avg = avg_distance\n                    max_distance_cluster_index = i\n\n        # split the largest cluster if its average distance is above the threshold\n        if max_distance_avg > 500:\n            largest_cluster = clusters.pop(max_distance_cluster_index)\n            centroids.pop(max_distance_cluster_index)  # Remove the centroid of the largest cluster\n\n            # split the biggest cluster with k_means with centroid\n            sub_clusters, sub_medoids = k_means(numpy.array(largest_cluster), 2, lower_interval_bound, upper_interval_bound)\n\n            # add the split clusters back to the main list of clusters\n            clusters.extend(sub_clusters)\n            centroids.extend(sub_medoids)\n        else:\n            break  # stop if all clusters are below the threshold\n\n\n\n\n    return clusters, centroids\n \n\n# !START HERE!\n\n# important variables\nnum_points = 20\nlower_interval_bound = -5000\nupper_interval_bound = 5000\n\nnum_additional_points = 40000\nclose_to_edge_penalty = 10\n\n# inicialize first points\npoints = generate_start_points(num_points, lower_interval_bound, upper_interval_bound) \n# find additional points\nadditional_points = generate_additional_points(points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty) \n\n# start divisive clustering\nclusters, centroids = divisive_clustering(additional_points, lower_interval_bound, upper_interval_bound)\n# print the average mean \nevaluate_cluster_average_mean(clusters, centroids, len(centroids), True)\nprint(\"buliding visualisation...\")\n# visualise clustering\nvisualise_clusters(clusters, centroids)\n"
    },
    {
        "prefix": "from matplotlib import pyplot as plt\nimport numpy\n\n# !FUNCTIONS!\n\ndef generate_start_points(num_points, lower_interval_bound, upper_interval_bound):\n    points_set = set() # use set so we have unique points\n\n    while len(points_set) < num_points: # untill points_set is not full - based on num_points\n        # generate random point (x,y)\n        x = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n        y = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n\n        points_set.add((x, y)) #add x and y as a tuple into the set\n\n    return numpy.array(list(points_set)) # convert set to numpy array and return\ndef generate_additional_points(default_points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty):\n    points = default_points.copy() #copy numpy array and work with its copy\n\n    for _ in range(num_additional_points):\n        # choose random point from existing points\n        random_index = numpy.random.randint(len(points))\n        random_point = points[random_index]\n        x, y = random_point\n\n        # edit offset interval if too close to edge\n        offset_interval = 100\n        if  x < lower_interval_bound + 100 or x > upper_interval_bound-100 or y < lower_interval_bound + 100 or y > upper_interval_bound-100:\n            offset_interval -= close_to_edge_penalty\n\n         # generate offset\n        X_offset = numpy.random.randint(-offset_interval, offset_interval)\n        Y_offset = numpy.random.randint(-offset_interval, offset_interval)\n\n\n        # set new point\n        new_x = x+X_offset\n        new_y = y+Y_offset\n\n        # append new point to the points\n        points = numpy.append(points, numpy.array([[new_x, new_y]]), axis = 0)\n\n    return points\ndef visualise_clusters(clusters, centroids):\n    # colors to choose from\n    colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\", \"pink\", \"brown\", \"cyan\", \"magenta\", \"lime\", \"indigo\", \"violet\", \"gold\", \"silver\", \"turquoise\", \"maroon\", \"navy\", \"coral\", \"teal\"]\n    plt.figure(figsize=(10, 10))\n\n     # for each cluster, show its points on matplotlib\n    for i in range(len(clusters)):\n        cluster = clusters[i]\n        if len(cluster) > 0:\n            cluster_points = numpy.array(cluster)\n            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i % len(colors)], s=10, label='Cluster {}'.format(i+1))\n\n    # Plot centroids with larger size and different color\n    centroids = numpy.array(centroids)\n    plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=100, marker='x', label='Centroids')\n\n    # aditional matplotlib settings\n    plt.xlim(-5000, 5000)\n    plt.ylim(-5000, 5000)\n    plt.title(\"K-Means Clustering (centroid)\")\n    plt.xlabel(\"X Coordinate\")\n    plt.ylabel(\"Y Coordinate\")\n    plt.legend()\n    plt.grid(False)\n    plt.show()\n\ndef euclidean_distance(point, centroid):\n    # calculate euclidian distance based on formula \n    return numpy.sqrt(numpy.sum(((centroid - point) ** 2)))\n\ndef k_means(points, k, lower_interval_bound, upper_interval_bound):\n    # choose k random centroids \n    centroids = []\n    for _ in range(k):\n        centroids.append([numpy.random.randint(lower_interval_bound, upper_interval_bound), numpy.random.randint(lower_interval_bound, upper_interval_bound)])\n\n    #asign points to the nearest centroid\n    i = 1\n    \n    current_avg = 0\n    previous_avg = 0\n\n    no_update_count = 0\n    average_distances = 0\n    average_distances_count = 0\n    while True:\n        print(f\"Iteration no.: {i}\")\n        i -=- 1\n\n        clusters = [[] for _ in range(k)] #define k clusters\n        distances_to_points = []\n        max_distances = [0] * k\n\n        # put each point to cluster\n        for point in points:\n            all_distances = [euclidean_distance(point, centroid) for centroid in centroids]\n\n            average_distances += sum(all_distances)\n            average_distances_count += len(all_distances)\n\n            closest_centroid = numpy.argmin(all_distances)\n            distances_to_points.append(all_distances[closest_centroid]) \n            clusters[closest_centroid].append(point)\n\n            # Update max distance for this centroid\n            max_distances[closest_centroid] = max(max_distances[closest_centroid], all_distances[closest_centroid])\n\n\n        # Calculate the average distance for this iteration\n        if distances_to_points:\n            average_distances = sum(distances_to_points)\n            average_distances_count = len(distances_to_points)\n            current_avg = average_distances / average_distances_count\n            print(f\"Average distance to closest centroid: {current_avg:.2f}\")\n\n        #recalculate centroids\n        new_centroids = []\n        for cluster in clusters:\n            if len(cluster) > 0:  # Only calculate the mean for non-empty clusters\n                new_centroids.append(numpy.mean(cluster, axis=0))\n            else:\n                new_centroids.append([numpy.random.randint(lower_interval_bound, upper_interval_bound), numpy.random.randint(lower_interval_bound, upper_interval_bound)])\n\n        if no_update_count > 0:\n            print(f\"No update for {no_update_count}/3\")\n\n        if abs(previous_avg - current_avg) < 1:\n            no_update_count += 1\n        else:\n            no_update_count = 0  # Reset if assignments have changed\n\n\n        # Stop if the maximum distance is less than 500 or if there's no update for 3 iterations\n        if no_update_count >= 3:\n            print(f\"    very small update for {no_update_count} iterations, not worth to continue\")\n            break\n\n        # Update centroids for the next iteration\n        centroids = new_centroids\n        previous_avg = current_avg\n\n    return clusters, centroids\n\ndef evaluate_cluster_average_mean(clusters, centroids, k):\n\n    succesful_clusters = 0\n\n    # calculate the sum of distances between each point in the cluster and the centroid of that cluster\n    for i, cluster in enumerate(clusters):\n        if len(cluster) > 0:\n            # Sum up the distances from each point in the cluster to its centroid\n            total_distance = sum(eu",
        "middle": "clidean_distance(point, centroids[i]) for point in cluster)\n            avg_di",
        "suffix": "stance = total_distance / len(cluster)\n            if avg_distance <= 500:\n                succesful_clusters += 1\n            print(f\"Average distance for cluster {i + 1}: {avg_distance:.2f}\")\n        else: \n            print(f\"Cluster {i+1} was left empty\")\n\n    print(f\"Number of cluster with average distance from centroid smaller than 500 is {succesful_clusters} from {k} possible\")\n\n    return\n####\n\n \n# !START HERE!\n\n#important variables\nnum_points = 20\nlower_interval_bound = -5000\nupper_interval_bound = 5000\n\nnum_additional_points = 40000\nclose_to_edge_penalty = 10\n\nk = 15 #number of clusters\n\n#inicialize first points\npoints = generate_start_points(num_points, lower_interval_bound, upper_interval_bound) \n# find additional points\nadditional_points = generate_additional_points(points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty) \n\nclusters, centroids = k_means(additional_points, k, lower_interval_bound, upper_interval_bound)\nevaluate_cluster_average_mean(clusters, centroids, k)\nprint(\"buliding visualisation...\")\nvisualise_clusters(clusters, centroids)\n"
    },
    {
        "prefix": "from matplotlib import pyplot as plt\nimport numpy\n\n# !FUNCTIONS!\n\ndef generate_start_points(num_points, lower_interval_bound, upper_interval_bound):\n    points_set = set() # use set so we have unique points\n\n    while len(points_set) < num_points: # untill points_set is not full - based on num_points\n        # generate random point (x,y)\n        x = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n        y = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n\n        points_set.add((x, y)) #add x and y as a tuple into the set\n\n    return numpy.array(list(points_set)) # convert set to numpy array and return\ndef generate_additional_points(default_points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty):\n    points = default_points.copy() #copy numpy array and work with its copy\n\n    for _ in range(num_additional_points):\n        # choose random point from existing points\n        random_index = numpy.random.randint(len(points))\n        random_point = points[random_index]\n        x, y = random_point\n\n        # edit offset interval if too close to edge\n        offset_interval = 100\n        if  x < lower_interval_bound + 100 or x > upper_interval_bound-100 or y < lower_interval_bound + 100 or y > upper_interval_bound-100:\n            offset_interval -= close_to_edge_penalty\n\n         # generate offset\n        X_offset = numpy.random.randint(-offset_interval, offset_interval)\n        Y_offset = numpy.random.randint(-offset_interval, offset_interval)\n\n\n        # set new point\n        new_x = x+X_offset\n        new_y = y+Y_offset\n\n        # append new point to the points\n        points = numpy.append(points, numpy.array([[new_x, new_y]]), axis = 0)\n\n    return points\ndef visualise_clusters(clusters, centroids):\n    # colors to choose from\n    colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\", \"pink\", \"brown\", \"cyan\", \"magenta\", \"lime\", \"indigo\", \"violet\", \"gold\", \"silver\", \"turquoise\", \"maroon\", \"navy\", \"coral\", \"teal\"]\n    plt.figure(figsize=(10, 10))\n\n     # for each cluster, show its points on matplotlib\n    for i in range(len(clusters)):\n        cluster = clusters[i]\n        if len(cluster) > 0:\n            cluster_points = numpy.array(cluster)\n            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i % len(colors)], s=10, label='Cluster {}'.format(i+1))\n\n    # Plot centroids with larger size and different color\n    centroids = numpy.array(centroids)\n    plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=100, marker='x', label='Centroids')\n\n    # aditional matplotlib settings\n    plt.xlim(-5000, 5000)\n    plt.ylim(-5000, 5000)\n    plt.title(\"K-Means Clustering (centroid)\")\n    plt.xlabel(\"X Coordinate\")\n    plt.ylabel(\"Y Coordinate\")\n    plt.legend()\n    plt.grid(False)\n    plt.show()\n\ndef euclidean_distance(point, centroid):\n    # calculate euclidian distance based on formula \n    return numpy.sqrt(numpy.sum(((centroid - point) ** 2)))\n\ndef k_means(points, k, lower_interval_bound, upper_interval_bound):\n    # choose k random centroids \n    centroids = []\n    for _ in range(k):\n        centroids.append([numpy.random.randint(lower_interval_bound, upper_interval_bound), numpy.random.randint(lower_interval_bound, upper_interval_bound)])\n\n    #asign points to the nearest centroid\n    i = 1\n    \n    current_avg = 0\n    previous_avg = 0\n\n    no_update_count = 0\n    average_distances = 0\n    average_distances_count = 0\n    while True:\n        print(f\"Iteration no.: {i}\")\n        i -=- 1\n\n        clusters = [[] for _ in range(k)] #define k clusters\n        distances_to_points = []\n        max_distances = [0] * k\n\n        # put each point to cluster\n        for point in points:\n            all_distances = [euclidean_distance(point, centroid) for centroid in centroids]\n\n            average_distances += sum(all_distances)\n            average_distances_count += len(all_distances)\n\n            closest_centroid = numpy.argmin(all_distances)\n            distances_to_points.append(all_distances[closest_centroid]) \n            clusters[closest_centroid].append(point)\n\n            # Update max distance for this centroid\n            max_distances[closest_centroid] = max(max_distances[closest_centroid], all_distances[closest_centroid])\n\n\n        # Calculate the average distance for this iteration\n        if distances_to_points:\n            average_distances = sum(distances_to_points)\n            average_distances_count =",
        "middle": " len(distances_to_points)\n           ",
        "suffix": " current_avg = average_distances / average_distances_count\n            print(f\"Average distance to closest centroid: {current_avg:.2f}\")\n\n        #recalculate centroids\n        new_centroids = []\n        for cluster in clusters:\n            if len(cluster) > 0:  # Only calculate the mean for non-empty clusters\n                new_centroids.append(numpy.mean(cluster, axis=0))\n            else:\n                new_centroids.append([numpy.random.randint(lower_interval_bound, upper_interval_bound), numpy.random.randint(lower_interval_bound, upper_interval_bound)])\n\n        if no_update_count > 0:\n            print(f\"No update for {no_update_count}/3\")\n\n        if abs(previous_avg - current_avg) < 1:\n            no_update_count += 1\n        else:\n            no_update_count = 0  # Reset if assignments have changed\n\n\n        # Stop if the maximum distance is less than 500 or if there's no update for 3 iterations\n        if no_update_count >= 3:\n            print(f\"    very small update for {no_update_count} iterations, not worth to continue\")\n            break\n\n        # Update centroids for the next iteration\n        centroids = new_centroids\n        previous_avg = current_avg\n\n    return clusters, centroids\n\ndef evaluate_cluster_average_mean(clusters, centroids, k):\n\n    succesful_clusters = 0\n\n    # calculate the sum of distances between each point in the cluster and the centroid of that cluster\n    for i, cluster in enumerate(clusters):\n        if len(cluster) > 0:\n            # Sum up the distances from each point in the cluster to its centroid\n            total_distance = sum(euclidean_distance(point, centroids[i]) for point in cluster)\n            avg_distance = total_distance / len(cluster)\n            if avg_distance <= 500:\n                succesful_clusters += 1\n            print(f\"Average distance for cluster {i + 1}: {avg_distance:.2f}\")\n        else: \n            print(f\"Cluster {i+1} was left empty\")\n\n    print(f\"Number of cluster with average distance from centroid smaller than 500 is {succesful_clusters} from {k} possible\")\n\n    return\n####\n\n \n# !START HERE!\n\n#important variables\nnum_points = 20\nlower_interval_bound = -5000\nupper_interval_bound = 5000\n\nnum_additional_points = 40000\nclose_to_edge_penalty = 10\n\nk = 15 #number of clusters\n\n#inicialize first points\npoints = generate_start_points(num_points, lower_interval_bound, upper_interval_bound) \n# find additional points\nadditional_points = generate_additional_points(points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty) \n\nclusters, centroids = k_means(additional_points, k, lower_interval_bound, upper_interval_bound)\nevaluate_cluster_average_mean(clusters, centroids, k)\nprint(\"buliding visualisation...\")\nvisualise_clusters(clusters, centroids)\n"
    },
    {
        "prefix": "import torch.nn as nn  # all neural network modules\nimport torch.nn.functional as functional\n\n\nclass NN (nn.Module): # inherits nn.Module (NN = Neural Network)\n    def __init__(self, input_size, num_classes): # for mnist it is 28*28 = 784\n        super(NN, self).__init__()\n\n        # self.hidden_layer_number = 50\n        # self.fc1 = nn.Linear(input_size, self.hidden_layer_number) # 50 - random number for the hidden layer\n        # self.fc2 = nn.Linear(self.hidden_layer_number, num_classes)\n\n        # two hidden layers\n  ",
        "middle": "      self.fc1 = nn.Linear(input_size, 250)\n    ",
        "suffix": "    self.fc2 = nn.Linear(250, 60)\n        self.fc3 = nn.Linear(60, num_classes)\n\n    def forward(self, tensor):\n        # tensor = functional.relu(self.fc1(tensor))\n        # tensor = self.fc2(tensor)\n        # return tensor\n        # return self.fc2(functional.relu(self.fc1(tensor)))\n        return self.fc3(functional.tanh(self.fc2(functional.tanh(self.fc1(tensor)))))\n"
    },
    {
        "prefix": "from matplotlib import pyplot as plt\nimport numpy\n\n# !FUNCTIONS!\n\ndef generate_start_points(num_points, lower_interval_bound, upper_interval_bound):\n    points_set = set() #use set so we have unique points\n\n    while len(points_set) < num_points: #untill points_set is not full - based on num_points\n        #generate random point (x,y)\n        x = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n        y = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n\n        points_set.add((x, y)) #add x and y as a tuple into the set\n\n    return numpy.array(list(points_set)) # convert set to numpy array and return\ndef generate_additional_points(default_points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty):\n    points = default_points.copy() #copy numpy array and work with its copy\n\n    for _ in range(num_additional_points):\n        # choose random point from existing points\n        random_index = numpy.random.randint(len(points)) \n        random_point = points[random_index]\n        x, y = random_point\n\n        # edit offset interval if too close to edge\n        offset_interval = 100\n        if  x < lower_interval_bound + 100 or x > upper_interval_bound-100 or y < lower_interval_bound + 100 or y > upper_interval_bound-100:\n            offset_interval -= close_to_edge_penalty\n\n        # generate offset\n        X_offset = numpy.random.randint(-offset_interval, offset_interval)\n        Y_offset = numpy.random.randint(-offset_interval, offset_interval)\n\n        # set new point\n        new_x = x+X_offset\n        new_y = y+Y_offset\n\n        points = numpy.append(points, numpy.array([[new_x, new_y]]), axis = 0) # append new point to the points\n    return points\n\ndef visualise_clusters(clusters, medoids):\n    # colors to choose from\n    colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\", \"pink\", \"brown\", \"cyan\", \"magenta\", \"lime\", \"indigo\", \"violet\", \"gold\", \"silver\", \"turquoise\", \"maroon\", \"navy\", \"coral\", \"teal\"]\n    plt.figure(figsize=(10, 10))\n\n    # for each cluster, show one point on matplotlib\n    for i in range(len(clusters)):\n        cluster = clusters[i]\n        if len(cluster) > 0:\n            cluster_points = numpy.array(cluster)\n            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i % len(colors)], s=10, label='Cluster {}'.format(i+1))\n\n    # plot centroids with larger size and different color\n    medoids = numpy.array(medoids)\n    plt.scatter(medoids[:, 0], medoids[:, 1], c='black', s=100, marker='x', label='Centroids', alpha=0.7)\n\n    # aditional matplotlib settings\n    plt.xlim(-5000, 5000)\n    plt.ylim(-5000, 5000)\n    plt.title(\"Divisive CLustering (centroid - K-Means)\")\n    plt.xlabel(\"X Coordinate\")\n    plt.ylabel(\"Y Coordinate\")\n    plt.legend()\n    plt.grid(False)\n    plt.show()\n\ndef euclidean_distance(point, other_point):\n    # calculate euclidian distance based on formula \n    return numpy.sqrt(numpy.sum(((other_point - point) ** 2)))\n\ndef evaluate_cluster_average_mean(clusters, centroids, k, print_logs):\n    ## if print_logs is False -> this function is used for calculation during divisive clustering\n    ## else if print_logs is True -> this function is used in the end of code to evaluare average mean\n\n    succesful_clusters = 0\n\n    # calculate the sum of distances between each point in the cluster and the centroid of that cluster\n    for i, cluster in enumerate(clusters):\n        if len(cluster) > 0:\n            # Sum up the distances from each point in the cluster to its centroid\n            total_distance = sum(euclidean_distance(point, centroids[i]) for point in cluster)\n            avg_distance = total_distance / len(cluster)\n            if avg_distance <= 500:\n                succesful_clusters += 1\n            if print_logs:\n                print(f\"Average distance for cluster {i + 1}: {avg_distance:.2f}\")\n        else: \n            if print_logs:\n                print(f\"Cluster {i+1} was left empty\")\n\n    if print_logs:\n        print(f\"Number of cluster with average distance from centroid smaller than 500 is {succesful_clusters} from {k} possible\")\n\n    return succesful_clusters, k\n\ndef k_means(points, k, lower_interval_bound, upper_interval_bound):\n    # choose k random centroids\n    centroids = points[numpy.random.choice(points.shape[0], k, replace=False)]\n\n    #asign points to the nearest centroid\n    i = 1\n    \n    current_avg = 0\n    previous_avg = 0\n\n    no_update_count = 0\n    average_distances = 0\n    average_distances_count = 0\n    while True:\n        print(f\"Iteration no.: {i}\")\n        i -=- 1\n\n        clusters = [[] for _ in range(k)] #define k clusters\n        distances_to_points = []\n        max_distances = [0] * k\n\n        # put each point to cluster\n        for point in points:\n            all_distances = [euclidean_distance(point, centroid) for centroid in centroids]\n\n            average_distances += sum(all_distances)\n            average_distances_count += len(all_distances)\n\n            closest_centroid = numpy.argmin(all_distances)\n            distances_to_points.append(all_distances[closest_centroid]) \n            clusters[closest_centroid].append(point)\n\n            # Update max distance for this centroid\n            max_distances[closest_centroid] = max(max_distances[closest_centroid], all_distances[closest_centroid])\n\n        #recalculate centroids\n        new_centroids = []\n        for cluster in clusters:\n            if len(cluster) > 0:  # Only calculate the mean for non-empty clusters\n                new_centroids.append(numpy.mean(cluster, axis=0))\n            else:\n                # print(\"bububu\")\n                new_centroids.append([numpy.random.randint(lower_interval_bound, upper_interval_bound), numpy.random.randint(lower_interval_bound, upper_interval_bound)])\n\n        if abs(previous_avg - current_avg) < 1: \n            no_update_count += 1\n        else:\n            no_update_count = 0  # Reset if assignments changed\n\n\n        # Stop if there's no update for 3 iterations\n        if no_update_count >= 3:\n            break\n\n        # Update centroids for the next iteration\n        centroids = new_centroids\n        previous_avg = current_avg\n\n    return clusters, centroids\n####\n\ndef divisive_clustering(points, lower_interval_bound, upper_interval_bound):\n\n    clusters = [points] # all points are one cluster at first\n    ",
        "middle": "centroids = [numpy.mean(points, axis=0)]\n\n    while True:\n        # Evaluate the current clusters\n  ",
        "suffix": "      successful_clusters, total_clusters = evaluate_cluster_average_mean(clusters, centroids, len(clusters), False)\n        \n        # if each clusters average mean is less than 500, stop the clustering \n        if successful_clusters == total_clusters:\n            print(\"All clusters have average less than 500.\")\n            break\n\n        # find the biggest cluster by average distance to its centroid\n        max_distance_cluster_index = -1\n        max_distance_avg = -1\n        for i, cluster in enumerate(clusters):\n            if len(cluster) > 0:\n                centroid = centroids[i]\n                avg_distance = sum(euclidean_distance(point, centroid) for point in cluster) / len(cluster)\n                if avg_distance > max_distance_avg:\n                    max_distance_avg = avg_distance\n                    max_distance_cluster_index = i\n\n        # split the largest cluster if its average distance is above the threshold\n        if max_distance_avg > 500:\n            largest_cluster = clusters.pop(max_distance_cluster_index)\n            centroids.pop(max_distance_cluster_index)  # Remove the centroid of the largest cluster\n\n            # split the biggest cluster with k_means with centroid\n            sub_clusters, sub_medoids = k_means(numpy.array(largest_cluster), 2, lower_interval_bound, upper_interval_bound)\n\n            # add the split clusters back to the main list of clusters\n            clusters.extend(sub_clusters)\n            centroids.extend(sub_medoids)\n        else:\n            break  # stop if all clusters are below the threshold\n\n\n\n\n    return clusters, centroids\n \n\n# !START HERE!\n\n# important variables\nnum_points = 20\nlower_interval_bound = -5000\nupper_interval_bound = 5000\n\nnum_additional_points = 40000\nclose_to_edge_penalty = 10\n\n# inicialize first points\npoints = generate_start_points(num_points, lower_interval_bound, upper_interval_bound) \n# find additional points\nadditional_points = generate_additional_points(points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty) \n\n# start divisive clustering\nclusters, centroids = divisive_clustering(additional_points, lower_interval_bound, upper_interval_bound)\n# print the average mean \nevaluate_cluster_average_mean(clusters, centroids, len(centroids), True)\nprint(\"buliding visualisation...\")\n# visualise clustering\nvisualise_clusters(clusters, centroids)\n"
    },
    {
        "prefix": "from matplotlib import pyplot as plt\nimport numpy\n\n# !FUNCTIONS!\n\ndef generate_start_points(num_points, lower_interval_bound, upper_interval_bound):\n    points_set = set() # use set so we have unique points\n\n    while len(points_set) < num_points: #untill points_set is not full - based on num_points\n        #generate random point (x,y)\n        x = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1) \n        y = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n\n        points_set.add((x, y)) #add x and y as a tuple into the set\n\n    return numpy.array(list(points_set))  # convert set to numpy array and return\ndef generate_additional_points(default_points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty):\n    points = default_points.copy() #copy numpy array and work with its copy\n\n    for _ in range(num_additional_points):\n        # choose random point from existing points\n        random_index = numpy.random.randint(len(points))\n        random_point = points[random_index]\n        x, y = random_point\n\n        # edit offset interval if too close to edge\n        offset_interval = 100\n        if  x < lower_interval_bound + 100 or x > upper_interval_bound-100 or y < lower_interval_bound + 100 or y > upper_interval_bound-100:\n            offset_interval -= close_to_edge_penalty\n\n        # generate offset\n        X_offset = numpy.random.randint(-offset_interval, offset_interval)\n        Y_offset = numpy.random.randint(-offset_interval, offset_interval)\n\n        # set new point\n        new_x = x+X_offset\n        new_y = y+Y_offset\n\n        # append new point to the points\n        points = numpy.append(points, numpy.array([[new_x, new_y]]), axis = 0)\n\n    return points\ndef visualise_clusters(clusters, medoids):\n    # colors to choose from\n    colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\", \"pink\", \"brown\", \"cyan\", \"magenta\", \"lime\", \"indigo\", \"violet\", \"gold\", \"silver\", \"turquoise\", \"maroon\", \"navy\", \"coral\", \"teal\"]\n    plt.figure(figsize=(10, 10))\n\n    # for each cluster, show one point on matplotlib\n    for i in range(len(clusters)):\n        cluster = clusters[i]\n        if len(cluster) > 0:\n            cluster_points = numpy.array(cluster)\n            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i % len(colors)], s=10, label='Cluster {}'.format(i+1))\n\n    # Plot centroids with larger size and different color\n    medoids = numpy.array(medoids)\n    plt.scatter(medoids[:, 0], medoids[:, 1], c='black', s=100, marker='x', label='Centroids', alpha=0.7)\n\n    # aditional matplotlib settings\n    plt.xlim(-5000, 5000)\n    plt.ylim(-5000, 5000)\n    plt.title(\"K-Means Clustering (medoid)\")\n    plt.xlabel(\"X Coordinate\")\n    plt.ylabel(\"Y Coordinate\")\n    plt.legend()\n    plt.grid(False)\n    plt.show()\n\ndef euclidean_distance(point, other_point):\n    # calculate euclidian distance based on formula \n    return numpy.sqrt(numpy.sum(((other_point - point) ** 2)))\n\ndef k_means(points, k, lower_interval_bound, upper_interval_bound):\n    # choose k random medoids \n    medoids = []\n    for _ in range(k):\n        medoids.append([numpy.random.randint(lower_interval_bound, upper_interval_bound), numpy.random.randint(lower_interval_bound, upper_interval_bound)])\n\n    #asign points to the nearest centroid\n    i = 1\n    \n    current_avg = 0\n    previous_avg = 0\n\n    no_update_count = 0\n    average_distances = 0\n    average_distances_count = 0\n    while True:\n  ",
        "middle": "      print(f\"Iteration no.: {i}\")\n        i -=- 1\n\n        clusters = [[] ",
        "suffix": "for _ in range(k)] #define k clusters\n        distances_to_medoids = []\n\n        # put each point to cluster\n        for point in points:\n            all_distances = [euclidean_distance(point, medoid) for medoid in medoids]\n\n            average_distances += sum(all_distances)\n            average_distances_count += len(all_distances)\n\n            closest_medoid = numpy.argmin(all_distances)\n            distances_to_medoids.append(all_distances[closest_medoid])\n            clusters[closest_medoid].append(point)\n\n        # Calculate the average distance for this iteration\n        if distances_to_medoids:\n            average_distances = sum(distances_to_medoids)\n            average_distances_count = len(distances_to_medoids)\n            current_avg = average_distances / average_distances_count\n            print(f\"Average distance to closest medoid: {current_avg:.2f}\")\n\n        #recalculate medoids\n        new_medoids = []\n        for cluster in clusters:\n            if len(cluster) > 0:\n                cluster_mean = numpy.mean(cluster, axis=0)\n                medoid = min(cluster, key=lambda point: euclidean_distance(point, cluster_mean))\n                new_medoids.append(medoid)\n            else:\n                new_medoids.append(points[numpy.random.randint(len(points))])\n\n        if no_update_count > 0:\n            print(f\"No update for {no_update_count}/3\")\n\n        if abs(previous_avg - current_avg) < 1:\n            no_update_count += 1\n        else:\n            no_update_count = 0  # Reset if assignments have changed\n\n\n        # Stop if there's no update for 3 iterations\n        if no_update_count >= 3:\n            print(f\"    very small update for {no_update_count} iterations, not worth to continue\")\n            break\n\n        # Update centroids for the next iteration\n        medoids = new_medoids\n        previous_avg = current_avg\n\n    return clusters, medoids\n\ndef evaluate_cluster_average_mean(clusters, centroids, k):\n\n    succesful_clusters = 0\n\n    # calculate the sum of distances between each point in the cluster and the centroid of that cluster\n    for i, cluster in enumerate(clusters):\n        if len(cluster) > 0:\n            # Sum up the distances from each point in the cluster to its centroid\n            total_distance = sum(euclidean_distance(point, centroids[i]) for point in cluster)\n            avg_distance = total_distance / len(cluster)\n            if avg_distance <= 500:\n                succesful_clusters += 1\n            print(f\"Average distance for cluster {i + 1}: {avg_distance:.2f}\")\n        else: \n            print(f\"Cluster {i+1} was left empty\")\n\n    print(f\"Number of cluster with average distance from centroid smaller than 500 is {succesful_clusters} from {k} possible\")\n\n    return\n####\n\n \n# !START HERE!\n\n#important variables\nnum_points = 20\nlower_interval_bound = -5000\nupper_interval_bound = 5000\n\nnum_additional_points = 40000\nclose_to_edge_penalty = 10\n\nk = 15 #number of clusters\n\n#inicialize first points\npoints = generate_start_points(num_points, lower_interval_bound, upper_interval_bound) \n# find additional points\nadditional_points = generate_additional_points(points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty) \n\nclusters, medoids = k_means(additional_points, k, lower_interval_bound, upper_interval_bound)\nevaluate_cluster_average_mean(clusters, medoids, k)\nprint(\"buliding visualisation...\")\nvisualise_clusters(clusters, medoids)\n"
    },
    {
        "prefix": "from matplotlib import pyplot as plt\nimport numpy\n\n# !FUNCTIONS!\n\ndef generate_start_points(num_points, lower_interval_bound, upper_interval_bound):\n    points_set = set() # use set so we have unique points\n\n    while len(points_set) < num_points: #untill points_set is not full - based on num_points\n        #generate random point (x,y)\n        x = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1) \n        y = numpy.random.randint(lower_interval_bound, upper_interval_bound + 1)\n\n        points_set.add((x, y)) #add x and y as a tuple into the set\n\n    return numpy.array(list(points_set))  # convert set to numpy array and return\ndef generate_additional_points(default_points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty):\n    points = default_points.copy() #copy numpy array and work with its copy\n\n    for _ in range(num_additional_points):\n        # choose random point from existing points\n        random_index = numpy.random.randint(len(points))\n        random_point = points[random_index]\n        x, y = random_point\n\n        # edit offset interval if too close to edge\n        offset_interval = 100\n        if  x < lower_interval_bound + 100 or x > upper_interval_bound-100 or y < lower_interval_bound + 100 or y > upper_interval_bound-100:\n            offset_interval -= close_to_edge_penalty\n\n        # generate offset\n        X_offset = numpy.random.randint(-offset_interval, offset_interval)\n        Y_offset = numpy.random.randint(-offset_interval, offset_interval)\n\n        # set new point\n        new_x = x+X_offset\n        new_y = y+Y_offset\n\n        # append new point to the points\n        points = numpy.append(points, numpy.array([[new_x, new_y]]), axis = 0)\n\n    return points\ndef visualise_clusters(clusters, medoids):\n    # colors to choose from\n    colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\", \"pink\", \"brown\", \"cyan\", \"magenta\", \"lime\", \"indigo\", \"violet\", \"gold\", \"silver\", \"turquoise\", \"maroon\", \"navy\", \"coral\", \"teal\"]\n    plt.figure(figsize=(10, 10))\n\n    # for each cluster, show one point on matplotlib\n    for i in range(len(clusters)):\n        cluster = clusters[i]\n        if len(cluster) > 0:\n            cluster_points = numpy.array(cluster)\n            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i % len(colors)], s=10, label='Cluster {}'.format(i+1))\n\n    # Plot centroids with larger size and different color\n    medoids = numpy.array(medoids)\n    plt.scatter(medoids[:, 0], medoids[:, 1], c='black', s=100, marker='x', label='Centroids', alpha=0.7)\n\n    # aditional matplotlib settings\n    plt.xlim(-5000, 5000)\n    plt.ylim(-5000, 5000)\n    plt.title(\"K-Means Clustering (medoid)\")\n    plt.xlabel(\"X Coordinate\")\n    plt.ylabel(\"Y Coordinate\")\n    plt.legend()\n    plt.grid(False)\n    plt.show()\n\ndef euclidean_distance(point, other_point):\n    # calculate euclidian distance based on formula \n    return numpy.sqrt(numpy.sum(((other_point - point) ** 2)))\n\ndef k_means(points, k, lower_interval_bound, upper_interval_bound):\n    # choose k random medoids \n    medoids = []\n    for _ in range(k):\n        medoids.append([numpy.random.randint(lower_interval_bound, upper_interval_bound), numpy.random.randint(lower_interval_bound, upper_interval_bound)])\n\n    #asign points to the nearest centroid\n    i = 1\n    \n    current_avg = 0\n    previous_avg = 0\n\n    no_update_count = 0\n    average_distances = 0\n    average_distances_count = 0\n    while True:\n        print(f\"Iteration no.: {i}\")\n        i -=- 1\n\n        clusters = [[] for _ in range(k)] #define k clusters\n        distances_to_medoids = []\n\n        # put each point to cluster\n        for point in points:\n            all_distances = [euclidean_distance(point, medoid) for medoid in medoids]\n\n            average_distances += sum(all_distances)\n            average_distances_count += len(all_distances)\n\n            closest_medoid = numpy.argmin(all_distances)\n            distances_to_medoids.append(all_distances[closest_medoid])\n            clusters[closest_medoid].append(point)\n\n        # Calculate the average distance for this iteration\n        if distances_to_medoids:\n            average_distances = sum(distances_to_medoids)\n            average_distances_count = len(distances_to_medoids)\n            current_avg = average_distances / average_distances_count\n            print(f\"Average distance to closest medoid: {current_avg:.2f}\")\n\n        #recalculate medoids\n        new_medoids = []\n        for cluster in clusters:\n            if len(cluster) > 0:\n                cluster_mean = numpy.mean(cluster, axis=0)\n                medoid = min(cluster, key=lambda point: euclidean_distance(point, cluster_mean))\n                new_medoids.append(medoid)\n            else:\n                new_medoids.append(points[numpy.random.randint(len(points))])\n\n        if no_update_count > 0:\n            print(f\"No update for {no_update_count}/3\")\n\n        if abs(previous_avg - current_avg) < 1:\n            no_update_count += 1\n        else:\n            no_update_count = 0  # Reset if assignments have changed\n\n\n        # Stop if there's no update for 3 iterations\n        if no_update_count >= 3:\n            print(f\"    very small update for {no_update_count} iterations, not worth to continue\")\n            break\n\n        # Update centroids for the next iteration\n        medoids = new_medoids\n        previous_avg = current_avg\n\n    return clusters, medoids\n\ndef evaluate_cluster_average_mean(clusters, centroids, k):\n\n    succesful_clusters = 0\n\n    # calculate the sum of distances between each point in the cluster and the centroid of that cluster\n    for i, cluster in enumerate(clusters):\n        if len(cluster) > 0:\n            # Sum up the distances from each point in the cluster to its centroid\n            total_distance = sum(euclidean_distance(point, centroids[i]) for point in cluster)\n            avg_distance = total_distance / len(cluster)\n            if avg_distance <= 500:\n                succesful_clusters += 1\n            print(f\"Average distance for cluster {i + 1}: {avg_distance:.2f}\")\n        else: \n            print(f\"Cluster {i+1} was left empty\")\n\n    print(f\"Number of cluster with average distance from centroid smaller than 500 is {succesful_clusters} from {k} possible\")\n\n    return\n####\n\n \n# !START HERE!\n\n#important variables\nnum_points = 20\nlower_interval_bound = -5000\nupper_interval_bound = 5000\n\nnum_additional_points = 40000\nclose_to_edge_penalty = 10\n\nk = 15 #number of clusters\n\n#inicialize first points\npoints = generate_start_points(num_points, lower_interval_bound, upper_interval_bound) \n# find additional points\nadditional_points = generate_additional_points(points, num_additional_points, lower_interval_bound, upper_interval_bound, close_to_edge_penalty) \n\nclusters, medoids = k_means(additional_points, k, lower_interval_bound, upper_interval_bound",
        "middle": ")\nevaluate_cluster_average_mean(clusters, medoids, k)\nprint(\"bul",
        "suffix": "iding visualisation...\")\nvisualise_clusters(clusters, medoids)\n"
    }
]